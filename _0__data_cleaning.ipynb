{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "357f9e76-7248-4b5a-a825-5efb2bd0813a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA CLEANING & PREPARATION\n",
      "================================================================================\n",
      "Initial data loaded. Shape: (38697, 37)\n",
      "Columns: ['id_devis', 'num_devis', 'nom_devis', 'nom_agence', 'nom_filiale_zone', 'nom_region', 'statut_devis', 'fg_devis_emis', 'fg_devis_refuse', 'fg_devis_accepte']...\n",
      "\n",
      "ðŸ” IDENTIFYING TEST/TRAINING ACCOUNTS...\n",
      "  Analyzing high-volume, zero-conversion accounts...\n",
      "  Found 4 accounts with >20 quotes and 0% conversion\n",
      "  Top 5 by volume: ['CL00201682', 'CL00069020', 'CL00291534', 'CL00294368']\n",
      "\n",
      "  Analyzing quote frequency patterns...\n",
      "  Found 3734 accounts with >1 quote/day average\n",
      "  Most extreme: CL00271519 with 6.0 quotes/day\n",
      "\n",
      "  Checking for potential employee accounts...\n",
      "\n",
      "ðŸ“Š TEST ACCOUNT ANALYSIS:\n",
      "Total suspicious accounts identified: 9\n",
      "Known test account (CL00201682): 229 quotes, 0% conversion\n",
      "Highest volume zero-conversion: CL00201682 with 227 quotes\n",
      "Highest frequency: CL00271519 with 6.0 quotes/day\n",
      "\n",
      "ðŸ§¹ CREATING CLEAN DATASET...\n",
      "Original dataset: 38,697 quotes\n",
      "Clean dataset: 38,342 quotes\n",
      "Quotes removed: 355 (0.9%)\n",
      "\n",
      "ðŸ“ˆ BASELINE STATISTICS (CLEAN DATA):\n",
      "Total unique customers: 25,931\n",
      "Total quotes: 38,342\n",
      "Avg quotes per customer: 1.48\n",
      "Quote-level conversion rate: 31.2%\n",
      "\n",
      "Customer-level metrics:\n",
      "Customers with at least one sale: 10,677.0 (41.2%)\n",
      "Customers with no sales: 15,254.0 (58.8%)\n",
      "\n",
      "ðŸ“Š DISTRIBUTION ANALYSIS:\n",
      "Customers with 1 quote: 17,600 (67.9%)\n",
      "Customers with 2-3 quotes: 7,451 (28.7%)\n",
      "Customers with 4+ quotes: 880 (3.4%)\n",
      "\n",
      "ðŸ“… TEMPORAL ANALYSIS:\n",
      "Date range: 2023-01-02 to 2025-12-14\n",
      "\n",
      "ðŸ” DATA QUALITY CHECK:\n",
      "Missing target values: 9 (0.0%)\n",
      "Removed 9 rows with NaN target\n",
      "\n",
      "ðŸ’¾ Clean dataset saved to: cleaned_quote_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/data_hes_quotes_france_202512-2.csv')\n",
    "df.head()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA CLEANING & PREPARATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Assuming df is already loaded from your CSV\n",
    "print(f\"Initial data loaded. Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)[:10]}...\")  # Show first 10 columns\n",
    "\n",
    "# 1. Identify test accounts\n",
    "print(\"\\nðŸ” IDENTIFYING TEST/TRAINING ACCOUNTS...\")\n",
    "\n",
    "def identify_test_accounts(df):\n",
    "    \"\"\"Identify suspicious accounts for removal\"\"\"\n",
    "    \n",
    "    # Known test account from discovery\n",
    "    known_test = ['CL00201682']\n",
    "    \n",
    "    # Pattern 1: High volume, zero conversion\n",
    "    print(\"  Analyzing high-volume, zero-conversion accounts...\")\n",
    "    customer_stats = df.groupby('numero_compte').agg({\n",
    "        'fg_devis_accepte': ['count', 'mean']\n",
    "    })\n",
    "    customer_stats.columns = ['quote_count', 'conversion_rate']\n",
    "    \n",
    "    # Find accounts with >20 quotes and 0% conversion\n",
    "    high_volume_zero = customer_stats[\n",
    "        (customer_stats['quote_count'] > 20) & \n",
    "        (customer_stats['conversion_rate'] == 0)\n",
    "    ].sort_values('quote_count', ascending=False)\n",
    "    \n",
    "    print(f\"  Found {len(high_volume_zero)} accounts with >20 quotes and 0% conversion\")\n",
    "    if len(high_volume_zero) > 0:\n",
    "        print(f\"  Top 5 by volume: {high_volume_zero.index[:5].tolist()}\")\n",
    "    \n",
    "    # Pattern 2: Unrealistic quote frequencies (>1 quote per day average)\n",
    "    print(\"\\n  Analyzing quote frequency patterns...\")\n",
    "    df['dt_creation_devis'] = pd.to_datetime(df['dt_creation_devis'])\n",
    "    \n",
    "    customer_timelines = df.groupby('numero_compte').agg({\n",
    "        'dt_creation_devis': ['min', 'max', 'count']\n",
    "    })\n",
    "    customer_timelines.columns = ['first_quote', 'last_quote', 'quote_count']\n",
    "    \n",
    "    customer_timelines['days_active'] = (\n",
    "        customer_timelines['last_quote'] - customer_timelines['first_quote']\n",
    "    ).dt.days + 1\n",
    "    \n",
    "    # Avoid division by zero for single-quote customers\n",
    "    customer_timelines['quotes_per_day'] = np.where(\n",
    "        customer_timelines['days_active'] > 0,\n",
    "        customer_timelines['quote_count'] / customer_timelines['days_active'],\n",
    "        1  # Single quote on one day = 1 quote/day\n",
    "    )\n",
    "    \n",
    "    unrealistic_frequency = customer_timelines[\n",
    "        customer_timelines['quotes_per_day'] > 1\n",
    "    ].sort_values('quotes_per_day', ascending=False)\n",
    "    \n",
    "    print(f\"  Found {len(unrealistic_frequency)} accounts with >1 quote/day average\")\n",
    "    if len(unrealistic_frequency) > 0:\n",
    "        print(f\"  Most extreme: {unrealistic_frequency.index[0]} with {unrealistic_frequency.iloc[0]['quotes_per_day']:.1f} quotes/day\")\n",
    "    \n",
    "    # Pattern 3: Employees/Internal (simplified - would need HR data in reality)\n",
    "    print(\"\\n  Checking for potential employee accounts...\")\n",
    "    # This is a placeholder - in reality would cross-reference with HR database\n",
    "    # For now, look for patterns like company email domains, known employee names\n",
    "    employee_patterns = []\n",
    "    \n",
    "    # Combine all suspicious accounts\n",
    "    all_suspicious = list(set(\n",
    "        known_test + \n",
    "        high_volume_zero.index.tolist()[:10] +  # Top 10 high-volume, zero-conversion\n",
    "        unrealistic_frequency.index.tolist()[:5]  # Top 5 by frequency\n",
    "    ))\n",
    "    \n",
    "    return all_suspicious, high_volume_zero, unrealistic_frequency\n",
    "\n",
    "# Run test account identification\n",
    "test_accounts, high_volume_zero, unrealistic_frequency = identify_test_accounts(df)\n",
    "\n",
    "print(f\"\\nðŸ“Š TEST ACCOUNT ANALYSIS:\")\n",
    "print(f\"Total suspicious accounts identified: {len(test_accounts)}\")\n",
    "print(f\"Known test account (CL00201682): 229 quotes, 0% conversion\")\n",
    "\n",
    "# Show impact of each pattern\n",
    "if len(high_volume_zero) > 0:\n",
    "    top_high_volume = high_volume_zero.iloc[0]\n",
    "    print(f\"Highest volume zero-conversion: {high_volume_zero.index[0]} with {int(top_high_volume['quote_count'])} quotes\")\n",
    "\n",
    "if len(unrealistic_frequency) > 0:\n",
    "    top_frequency = unrealistic_frequency.iloc[0]\n",
    "    print(f\"Highest frequency: {unrealistic_frequency.index[0]} with {top_frequency['quotes_per_day']:.1f} quotes/day\")\n",
    "\n",
    "# 2. Remove test accounts and create clean dataset\n",
    "print(\"\\nðŸ§¹ CREATING CLEAN DATASET...\")\n",
    "df_clean = df[~df['numero_compte'].isin(test_accounts)].copy()\n",
    "quotes_removed = len(df) - len(df_clean)\n",
    "percent_removed = (quotes_removed / len(df)) * 100\n",
    "\n",
    "print(f\"Original dataset: {len(df):,} quotes\")\n",
    "print(f\"Clean dataset: {len(df_clean):,} quotes\")\n",
    "print(f\"Quotes removed: {quotes_removed:,} ({percent_removed:.1f}%)\")\n",
    "\n",
    "# 3. Calculate baseline statistics\n",
    "print(\"\\nðŸ“ˆ BASELINE STATISTICS (CLEAN DATA):\")\n",
    "\n",
    "# Basic metrics\n",
    "total_customers = df_clean['numero_compte'].nunique()\n",
    "total_quotes = len(df_clean)\n",
    "quotes_per_customer = total_quotes / total_customers\n",
    "conversion_rate = df_clean['fg_devis_accepte'].mean()\n",
    "\n",
    "print(f\"Total unique customers: {total_customers:,}\")\n",
    "print(f\"Total quotes: {total_quotes:,}\")\n",
    "print(f\"Avg quotes per customer: {quotes_per_customer:.2f}\")\n",
    "print(f\"Quote-level conversion rate: {conversion_rate:.1%}\")\n",
    "\n",
    "# Customer-level conversion\n",
    "customer_conversion = df_clean.groupby('numero_compte')['fg_devis_accepte'].max()\n",
    "customer_conversion_rate = customer_conversion.mean()\n",
    "customers_with_sales = customer_conversion.sum()\n",
    "\n",
    "print(f\"\\nCustomer-level metrics:\")\n",
    "print(f\"Customers with at least one sale: {customers_with_sales:,} ({customer_conversion_rate:.1%})\")\n",
    "print(f\"Customers with no sales: {total_customers - customers_with_sales:,} ({1 - customer_conversion_rate:.1%})\")\n",
    "\n",
    "# Distribution analysis\n",
    "print(\"\\nðŸ“Š DISTRIBUTION ANALYSIS:\")\n",
    "quote_counts = df_clean['numero_compte'].value_counts()\n",
    "print(f\"Customers with 1 quote: {(quote_counts == 1).sum():,} ({(quote_counts == 1).sum()/total_customers:.1%})\")\n",
    "print(f\"Customers with 2-3 quotes: {((quote_counts >= 2) & (quote_counts <= 3)).sum():,} ({((quote_counts >= 2) & (quote_counts <= 3)).sum()/total_customers:.1%})\")\n",
    "print(f\"Customers with 4+ quotes: {(quote_counts >= 4).sum():,} ({(quote_counts >= 4).sum()/total_customers:.1%})\")\n",
    "\n",
    "# Temporal analysis\n",
    "print(\"\\nðŸ“… TEMPORAL ANALYSIS:\")\n",
    "df_clean['dt_creation_devis'] = pd.to_datetime(df_clean['dt_creation_devis'])\n",
    "print(f\"Date range: {df_clean['dt_creation_devis'].min().date()} to {df_clean['dt_creation_devis'].max().date()}\")\n",
    "\n",
    "# Check data quality issues\n",
    "print(\"\\nðŸ” DATA QUALITY CHECK:\")\n",
    "missing_target = df_clean['fg_devis_accepte'].isna().sum()\n",
    "print(f\"Missing target values: {missing_target} ({missing_target/len(df_clean):.1%})\")\n",
    "\n",
    "# Remove any remaining NaN in target (if any)\n",
    "if missing_target > 0:\n",
    "    df_clean = df_clean.dropna(subset=['fg_devis_accepte'])\n",
    "    print(f\"Removed {missing_target} rows with NaN target\")\n",
    "\n",
    "# 4. Save clean dataset for Day 2\n",
    "output_file = 'cleaned_quote_data.csv'\n",
    "df_clean.to_csv(output_file, index=False)\n",
    "print(f\"\\nðŸ’¾ Clean dataset saved to: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
