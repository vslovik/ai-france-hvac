{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e8872fd-deb9-4217-9338-a3a2ed4cc89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ STARTING ENHANCED DATA DISCOVERY\n",
      "================================================================================\n",
      "================================================================================\n",
      "ENHANCED DATA DISCOVERY ANALYSIS - UPGRADED DATASET\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ DATASET STRUCTURE ANALYSIS\n",
      "------------------------------------------------------------\n",
      "â€¢ df_quotes shape: (34014, 47)\n",
      "â€¢ df_quotes columns: 47\n",
      "âœ… Using customer ID column: 'numero_compte'\n",
      "âœ… Unique customers in df_quotes: 23,888\n",
      "\n",
      "\n",
      "ðŸ“Š 1. CUSTOMER SEGMENT ANALYSIS\n",
      "------------------------------------------------------------\n",
      "âœ… Found client status column: 'statut_client'\n",
      "\n",
      "ðŸ“ˆ Client Segment Distribution:\n",
      "   â€¢ NEW_PROSPECT: 25,209 quotes (74.1%)\n",
      "   â€¢ OLD_PROSPECT: 4,866 quotes (14.3%)\n",
      "   â€¢ ACTIVE_CUSTOMER: 3,939 quotes (11.6%)\n",
      "\n",
      "ðŸŽ¯ Conversion Rates by Segment:\n",
      "   â€¢ ACTIVE_CUSTOMER: 35.5% conversion rate\n",
      "   â€¢ NEW_PROSPECT: 28.6% conversion rate\n",
      "   â€¢ OLD_PROSPECT: 32.6% conversion rate\n",
      "\n",
      "ðŸ“ Quote Patterns by Segment:\n",
      "   Quote statistics by segment:\n",
      "   â€¢ ACTIVE_CUSTOMER: 3,939 quotes, 2,703 customers (1.46 quotes/customer)\n",
      "   â€¢ NEW_PROSPECT: 25,209 quotes, 18,286 customers (1.38 quotes/customer)\n",
      "   â€¢ OLD_PROSPECT: 4,866 quotes, 3,548 customers (1.37 quotes/customer)\n",
      "\n",
      "\n",
      "ðŸ“ˆ 2. ENGAGEMENT SIGNAL ANALYSIS\n",
      "------------------------------------------------------------\n",
      "ðŸ“Š Customer Engagement Patterns:\n",
      "   â€¢ Average quotes per customer: 1.42\n",
      "   â€¢ Median quotes per customer: 1.0\n",
      "   â€¢ Max quotes for a single customer: 22\n",
      "   â€¢ Average engagement days: 115.8\n",
      "\n",
      "ðŸ”¢ Single vs Multi-Quote Customers:\n",
      "   â€¢ Single-quote customers: 16,736 (70.1%)\n",
      "   â€¢ Multi-quote customers: 7,152 (29.9%)\n",
      "\n",
      "ðŸŽ¯ Conversion by Engagement Pattern:\n",
      "   â€¢ Single-quote customers: 37.3% conversion\n",
      "   â€¢ Multi-quote customers: 44.9% conversion\n",
      "   âš¡ INSIGHT: Multi-quote customers convert 7.5% HIGHER!\n",
      "\n",
      "\n",
      "ðŸ’° 3. PRICE SIGNAL ANALYSIS\n",
      "------------------------------------------------------------\n",
      "âŒ No price column found for price analysis\n",
      "\n",
      "\n",
      "ðŸ·ï¸ 4. BRAND SIGNAL ANALYSIS\n",
      "------------------------------------------------------------\n",
      "âœ… Found brand column: 'marque_produit'\n",
      "\n",
      "ðŸ† Top 10 Brands by Quote Volume:\n",
      "    1. MITSUBISHI ELECTRIC        6,202 quotes (18.2% share)\n",
      "    2. ATLANTIC                   5,613 quotes (16.5% share)\n",
      "    3. MCZ                        3,345 quotes (9.8% share)\n",
      "    4. FRISQUET                   2,300 quotes (6.8% share)\n",
      "    5. E.L.M. LEBLANC             2,213 quotes (6.5% share)\n",
      "    6. SAUNIER DUVAL              1,654 quotes (4.9% share)\n",
      "    7. HITACHI                    1,589 quotes (4.7% share)\n",
      "    8. DE DIETRICH                1,250 quotes (3.7% share)\n",
      "    9. VIESSMANN                  1,197 quotes (3.5% share)\n",
      "   10. SOL&MOI                      902 quotes (2.7% share)\n",
      "\n",
      "ðŸŽ¯ Top 10 Brands by Conversion Rate (min 50 quotes):\n",
      "    1. INCONNU                    58.6% conversion (266 quotes)\n",
      "    2. MCZ                        39.7% conversion (3,345 quotes)\n",
      "    3. MDD                        38.1% conversion (349 quotes)\n",
      "    4. CHAPPEE                    38.1% conversion (657 quotes)\n",
      "    5. FINIMETAL                  37.7% conversion (61 quotes)\n",
      "    6. DE DIETRICH                35.5% conversion (1,250 quotes)\n",
      "    7. DROPSON                    35.2% conversion (54 quotes)\n",
      "    8. DOVRE                      35.2% conversion (762 quotes)\n",
      "    9. DENIA                      33.3% conversion (120 quotes)\n",
      "   10. THERMOR                    32.4% conversion (327 quotes)\n",
      "\n",
      "âš ï¸  Bottom 5 Converting Brands (min 50 quotes):\n",
      "    1. BOSCH                      14.2% conversion (219 quotes)\n",
      "    2. THERMOROSSI                14.7% conversion (68 quotes)\n",
      "    3. CADEL                      15.9% conversion (107 quotes)\n",
      "    4. SOL&MOI                    16.1% conversion (902 quotes)\n",
      "    5. VAILLANT                   19.2% conversion (99 quotes)\n",
      "\n",
      "\n",
      "âš™ï¸ 5. EQUIPMENT SIGNAL ANALYSIS\n",
      "------------------------------------------------------------\n",
      "âœ… Found equipment column: 'regroup_famille_equipement_produit'\n",
      "\n",
      "ðŸ”§ Top 10 Equipment Categories:\n",
      "    1. BOILER_GAS                           9,937 quotes (29.2%)\n",
      "    2. STOVE                                7,432 quotes (21.8%)\n",
      "    3. AIR_CONDITIONER                      6,664 quotes (19.6%)\n",
      "    4. HEAT_PUMP                            6,370 quotes (18.7%)\n",
      "    5. OTHER                                3,611 quotes (10.6%)\n",
      "\n",
      "ðŸŽ¯ Equipment Conversion Analysis (min 50 quotes):\n",
      "    1. STOVE                                33.2% conversion (7,432 quotes)\n",
      "    2. HEAT_PUMP                            32.3% conversion (6,370 quotes)\n",
      "    3. BOILER_GAS                           31.3% conversion (9,937 quotes)\n",
      "    4. OTHER                                27.8% conversion (3,611 quotes)\n",
      "    5. AIR_CONDITIONER                      23.6% conversion (6,664 quotes)\n",
      "\n",
      "\n",
      "ðŸ”„ 6. PROCESS ADOPTION ANALYSIS\n",
      "------------------------------------------------------------\n",
      "âœ… Found process column: 'fg_nouveau_process_relance_devis'\n",
      "\n",
      "ðŸ“Š Process Distribution:\n",
      "   â€¢ New Process: 19,644 quotes (57.8%)\n",
      "   â€¢ Old Process: 14,370 quotes (42.2%)\n",
      "\n",
      "ðŸŽ¯ Process Conversion Analysis:\n",
      "   â€¢ Old Process: 32.5% conversion\n",
      "   â€¢ New Process: 28.2% conversion\n",
      "   âš¡ IMPACT: New process shows -4.4% conversion change\n",
      "\n",
      "\n",
      "ðŸ‘¥ 7. COMMERCIAL ROLE ANALYSIS\n",
      "------------------------------------------------------------\n",
      "âœ… Found commercial role column: 'fonction_commercial'\n",
      "\n",
      "ðŸ“Š Top 10 Commercial Roles:\n",
      "    1. Commercial                     23,052 quotes (67.8%)\n",
      "    2. Technico-Commercial             2,505 quotes (7.4%)\n",
      "    3. Assistante                      1,239 quotes (3.6%)\n",
      "    4. Technico Commercial               544 quotes (1.6%)\n",
      "    5. Admin SAV                         360 quotes (1.1%)\n",
      "    6. Assistante Administrative         304 quotes (0.9%)\n",
      "    7. Responsable                       303 quotes (0.9%)\n",
      "    8. ContrÃ´le de gestion               253 quotes (0.7%)\n",
      "    9. Contre maÃ®tre                     238 quotes (0.7%)\n",
      "   10. rÃ©fÃ©rent technique                220 quotes (0.6%)\n",
      "\n",
      "ðŸŽ¯ Top Converting Roles (min 30 quotes):\n",
      "    1. Technico Commercial             54.8% conversion (544 quotes)\n",
      "    2. Technicien de Maintenance       50.0% conversion (42 quotes)\n",
      "    3. Dirigeante                      45.0% conversion (40 quotes)\n",
      "    4. Administration                  38.5% conversion (39 quotes)\n",
      "    5. Assistante                      36.9% conversion (1,239 quotes)\n",
      "    6. Technicien                      34.8% conversion (46 quotes)\n",
      "    7. ContrÃ´le de gestion             33.2% conversion (253 quotes)\n",
      "    8. Admin SAV                       30.3% conversion (360 quotes)\n",
      "    9. Technicien chauffagiste         30.1% conversion (209 quotes)\n",
      "   10. Commercial                      29.6% conversion (23,052 quotes)\n",
      "\n",
      "\n",
      "ðŸ¢ 8. AGENCY PERFORMANCE ANALYSIS\n",
      "------------------------------------------------------------\n",
      "âœ… Found agency column: 'nom_agence'\n",
      "\n",
      "ðŸ“Š Top 10 Agencies by Quote Volume:\n",
      "    1. VB Gaz                     4,360 quotes (12.8%)\n",
      "    2. Agence Caen                3,724 quotes (10.9%)\n",
      "    3. Chauffage du Nord          3,214 quotes (9.4%)\n",
      "    4. SBF Energies               3,065 quotes (9.0%)\n",
      "    5. Agence Rouen               2,575 quotes (7.6%)\n",
      "    6. Agence Valognes            2,377 quotes (7.0%)\n",
      "    7. Roussin Energies           2,229 quotes (6.6%)\n",
      "    8. Agence Cherbourg           2,180 quotes (6.4%)\n",
      "    9. Agence Avranches           1,280 quotes (3.8%)\n",
      "   10. SMT Energies               1,068 quotes (3.1%)\n",
      "\n",
      "ðŸŽ¯ Agency Conversion Performance (min 100 quotes):\n",
      "\n",
      "ðŸ† Top 5 Converting Agencies:\n",
      "    1. Agence Evreux         41.8% conversion, 1,030 quotes, avg: N/A\n",
      "    2. Agence Quettehou      38.7% conversion,   812 quotes, avg: N/A\n",
      "    3. Lepretre Lievin       36.0% conversion,   211 quotes, avg: N/A\n",
      "    4. Agence Caen           35.1% conversion, 3,724 quotes, avg: N/A\n",
      "    5. Agence CondÃ© sur Vire  34.7% conversion,   421 quotes, avg: N/A\n",
      "\n",
      "âš ï¸  Bottom 5 Converting Agencies:\n",
      "    1. Lepretre Roclincourt  21.0% conversion,   415 quotes, avg: N/A\n",
      "    2. Aujard                22.1% conversion,   874 quotes, avg: N/A\n",
      "    3. Mure Energies         23.1% conversion,   987 quotes, avg: N/A\n",
      "    4. GD Energies           23.4% conversion,   674 quotes, avg: N/A\n",
      "    5. SBF Energies          26.5% conversion, 3,065 quotes, avg: N/A\n",
      "\n",
      "\n",
      "ðŸ“‹ 10. DATA QUALITY & INSIGHTS SUMMARY\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Dataset Overview:\n",
      "   â€¢ Total quotes in df_quotes: 34,014\n",
      "   â€¢ Unique customers: 23,888\n",
      "   â€¢ Overall conversion rate: 30.0%\n",
      "\n",
      "ðŸ” Missing Data Analysis (df_quotes):\n",
      "   Top columns with missing values:\n",
      "    1. lb_statut_preparation_chantier      34,014.0 missing (100.0%)\n",
      "    2. dt_visite_commerciale               27,957.0 missing (82.2%)\n",
      "    3. dth_emission_devis                  27,880.0 missing (82.0%)\n",
      "    4. id_opportunite                      26,420.0 missing (77.7%)\n",
      "    5. dt_facture_max                      23,524.0 missing (69.2%)\n",
      "    6. dt_facture_min                      23,524.0 missing (69.2%)\n",
      "    7. dt_signature_devis                  23,175.0 missing (68.1%)\n",
      "    8. dt_prem_contrat                     19,045.0 missing (56.0%)\n",
      "    9. nom_campagne                        4,931.0 missing (14.5%)\n",
      "   10. fonction_commercial                 3,952.0 missing (11.6%)\n",
      "\n",
      "ðŸ’¡ KEY INSIGHTS:\n",
      "   1. â€¢ Multi-quote customers convert 7.5% HIGHER than single-quote customers\n",
      "   2. â€¢ New process converts 4.4% WORSE than old process\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS COMPLETE - READY FOR FEATURE ENGINEERING OPTIMIZATION\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load original clean quote data\n",
    "df_quotes = pd.read_csv('cleaned_quote_data.csv')\n",
    "df_quotes['dt_creation_devis'] = pd.to_datetime(df_quotes['dt_creation_devis'])\n",
    "\n",
    "\n",
    "def enhanced_data_discovery_analysis(df_quotes, X_new_clean=None):\n",
    "    \"\"\"\n",
    "    Comprehensive data discovery focusing on new segments and predictive signals\n",
    "    Uses df_quotes for raw analysis, X_new_clean for feature analysis\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ENHANCED DATA DISCOVERY ANALYSIS - UPGRADED DATASET\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    analysis_results = {}\n",
    "    \n",
    "    # ========== DATA STRUCTURE CHECK ==========\n",
    "    print(\"\\nðŸ“ DATASET STRUCTURE ANALYSIS\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"â€¢ df_quotes shape: {df_quotes.shape}\")\n",
    "    print(f\"â€¢ df_quotes columns: {len(df_quotes.columns)}\")\n",
    "    \n",
    "    if X_new_clean is not None:\n",
    "        print(f\"â€¢ X_new_clean shape: {X_new_clean.shape}\")\n",
    "        print(f\"â€¢ X_new_clean columns: {len(X_new_clean.columns)}\")\n",
    "    \n",
    "    # Check for customer ID column\n",
    "    customer_id_col = 'numero_compte'\n",
    "    if customer_id_col not in df_quotes.columns:\n",
    "        # Try to find alternative customer ID column\n",
    "        possible_ids = ['customer_id', 'client_id', 'id_client', 'customer_number']\n",
    "        for col in possible_ids:\n",
    "            if col in df_quotes.columns:\n",
    "                customer_id_col = col\n",
    "                break\n",
    "        \n",
    "        if customer_id_col == 'numero_compte':\n",
    "            print(\"âŒ ERROR: No customer ID column found in df_quotes!\")\n",
    "            print(\"Available columns:\", list(df_quotes.columns))\n",
    "            return analysis_results\n",
    "    \n",
    "    print(f\"âœ… Using customer ID column: '{customer_id_col}'\")\n",
    "    print(f\"âœ… Unique customers in df_quotes: {df_quotes[customer_id_col].nunique():,}\")\n",
    "    \n",
    "    # ========== 1. CUSTOMER SEGMENT ANALYSIS ==========\n",
    "    print(\"\\n\\nðŸ“Š 1. CUSTOMER SEGMENT ANALYSIS\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Check for client status column\n",
    "    client_status_col = None\n",
    "    possible_cols = ['client_status', 'statut_client', 'fg_statut_client', 'status_client']\n",
    "    \n",
    "    for col in possible_cols:\n",
    "        if col in df_quotes.columns:\n",
    "            client_status_col = col\n",
    "            break\n",
    "    \n",
    "    if client_status_col:\n",
    "        print(f\"âœ… Found client status column: '{client_status_col}'\")\n",
    "        \n",
    "        # Segment distribution\n",
    "        segment_counts = df_quotes[client_status_col].value_counts()\n",
    "        segment_pct = df_quotes[client_status_col].value_counts(normalize=True) * 100\n",
    "        \n",
    "        print(f\"\\nðŸ“ˆ Client Segment Distribution:\")\n",
    "        for segment, count in segment_counts.items():\n",
    "            pct = segment_pct[segment]\n",
    "            print(f\"   â€¢ {segment}: {count:,} quotes ({pct:.1f}%)\")\n",
    "        \n",
    "        # Analyze conversion rates by segment\n",
    "        if 'fg_devis_accepte' in df_quotes.columns:\n",
    "            print(f\"\\nðŸŽ¯ Conversion Rates by Segment:\")\n",
    "            conversion_by_segment = df_quotes.groupby(client_status_col)['fg_devis_accepte'].mean() * 100\n",
    "            \n",
    "            for segment, conv_rate in conversion_by_segment.items():\n",
    "                print(f\"   â€¢ {segment}: {conv_rate:.1f}% conversion rate\")\n",
    "            \n",
    "            analysis_results['segment_conversion_rates'] = conversion_by_segment\n",
    "        \n",
    "        # Analyze quote patterns by segment\n",
    "        print(f\"\\nðŸ“ Quote Patterns by Segment:\")\n",
    "        quotes_by_segment = df_quotes.groupby(client_status_col)[customer_id_col].agg(['count', 'nunique'])\n",
    "        quotes_by_segment['avg_quotes_per_customer'] = quotes_by_segment['count'] / quotes_by_segment['nunique']\n",
    "        \n",
    "        print(f\"   Quote statistics by segment:\")\n",
    "        for segment in quotes_by_segment.index:\n",
    "            total_quotes = quotes_by_segment.loc[segment, 'count']\n",
    "            unique_customers = quotes_by_segment.loc[segment, 'nunique']\n",
    "            avg_quotes = quotes_by_segment.loc[segment, 'avg_quotes_per_customer']\n",
    "            print(f\"   â€¢ {segment}: {total_quotes:,} quotes, {unique_customers:,} customers ({avg_quotes:.2f} quotes/customer)\")\n",
    "    else:\n",
    "        print(\"âŒ No client status column found in df_quotes\")\n",
    "    \n",
    "    # ========== 2. ENGAGEMENT SIGNAL ANALYSIS ==========\n",
    "    print(\"\\n\\nðŸ“ˆ 2. ENGAGEMENT SIGNAL ANALYSIS\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Check date column for temporal analysis\n",
    "    date_col = 'dt_creation_devis'\n",
    "    if date_col in df_quotes.columns:\n",
    "        df_quotes[date_col] = pd.to_datetime(df_quotes[date_col], errors='coerce')\n",
    "        \n",
    "        # Engagement frequency analysis\n",
    "        engagement_stats = df_quotes.groupby(customer_id_col).agg(\n",
    "            total_quotes=(customer_id_col, 'size'),\n",
    "            engagement_days=(date_col, lambda x: (x.max() - x.min()).days if len(x) > 1 else 0)\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Calculate quotes per day for customers with multiple quotes\n",
    "        engagement_stats['quotes_per_day'] = engagement_stats.apply(\n",
    "            lambda x: x['total_quotes'] / max(x['engagement_days'], 1), axis=1\n",
    "        )\n",
    "        \n",
    "        print(\"ðŸ“Š Customer Engagement Patterns:\")\n",
    "        print(f\"   â€¢ Average quotes per customer: {engagement_stats['total_quotes'].mean():.2f}\")\n",
    "        print(f\"   â€¢ Median quotes per customer: {engagement_stats['total_quotes'].median():.1f}\")\n",
    "        print(f\"   â€¢ Max quotes for a single customer: {engagement_stats['total_quotes'].max()}\")\n",
    "        print(f\"   â€¢ Average engagement days: {engagement_stats[engagement_stats['engagement_days'] > 0]['engagement_days'].mean():.1f}\")\n",
    "        \n",
    "        # Single vs multi-quote analysis\n",
    "        single_quote_customers = (engagement_stats['total_quotes'] == 1).sum()\n",
    "        multi_quote_customers = (engagement_stats['total_quotes'] > 1).sum()\n",
    "        total_customers = len(engagement_stats)\n",
    "        \n",
    "        print(f\"\\nðŸ”¢ Single vs Multi-Quote Customers:\")\n",
    "        print(f\"   â€¢ Single-quote customers: {single_quote_customers:,} ({single_quote_customers/total_customers*100:.1f}%)\")\n",
    "        print(f\"   â€¢ Multi-quote customers: {multi_quote_customers:,} ({multi_quote_customers/total_customers*100:.1f}%)\")\n",
    "        \n",
    "        if 'fg_devis_accepte' in df_quotes.columns:\n",
    "            # Get customer-level conversion\n",
    "            customer_conversion = df_quotes.groupby(customer_id_col)['fg_devis_accepte'].max().reset_index()\n",
    "            engagement_stats = engagement_stats.merge(customer_conversion, on=customer_id_col)\n",
    "            \n",
    "            # Conversion rates by engagement pattern\n",
    "            conversion_single = engagement_stats[engagement_stats['total_quotes'] == 1]['fg_devis_accepte'].mean() * 100\n",
    "            conversion_multi = engagement_stats[engagement_stats['total_quotes'] > 1]['fg_devis_accepte'].mean() * 100\n",
    "            \n",
    "            print(f\"\\nðŸŽ¯ Conversion by Engagement Pattern:\")\n",
    "            print(f\"   â€¢ Single-quote customers: {conversion_single:.1f}% conversion\")\n",
    "            print(f\"   â€¢ Multi-quote customers: {conversion_multi:.1f}% conversion\")\n",
    "            \n",
    "            if conversion_single > conversion_multi:\n",
    "                print(f\"   âš¡ INSIGHT: Single-quote customers convert {conversion_single - conversion_multi:.1f}% HIGHER!\")\n",
    "            else:\n",
    "                print(f\"   âš¡ INSIGHT: Multi-quote customers convert {conversion_multi - conversion_single:.1f}% HIGHER!\")\n",
    "            \n",
    "            analysis_results['single_vs_multi_conversion'] = {\n",
    "                'single': conversion_single,\n",
    "                'multi': conversion_multi,\n",
    "                'difference': abs(conversion_single - conversion_multi)\n",
    "            }\n",
    "    else:\n",
    "        print(\"âŒ No date column found for engagement analysis\")\n",
    "    \n",
    "    # ========== 3. PRICE SIGNAL ANALYSIS ==========\n",
    "    print(\"\\n\\nðŸ’° 3. PRICE SIGNAL ANALYSIS\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    price_cols = ['total_price', 'montant_total', 'prix_total', 'montant_devis', 'montant_ttc']\n",
    "    price_col = None\n",
    "    \n",
    "    for col in price_cols:\n",
    "        if col in df_quotes.columns and df_quotes[col].notna().sum() > 0:\n",
    "            price_col = col\n",
    "            break\n",
    "    \n",
    "    if price_col:\n",
    "        # Clean price data\n",
    "        df_quotes[price_col] = pd.to_numeric(df_quotes[price_col], errors='coerce')\n",
    "        \n",
    "        print(f\"âœ… Found price column: '{price_col}'\")\n",
    "        print(f\"\\nðŸ“Š Price Statistics (all quotes):\")\n",
    "        print(f\"   â€¢ Mean price: â‚¬{df_quotes[price_col].mean():,.0f}\")\n",
    "        print(f\"   â€¢ Median price: â‚¬{df_quotes[price_col].median():,.0f}\")\n",
    "        print(f\"   â€¢ Min price: â‚¬{df_quotes[price_col].min():,.0f}\")\n",
    "        print(f\"   â€¢ Max price: â‚¬{df_quotes[price_col].max():,.0f}\")\n",
    "        print(f\"   â€¢ Price std: â‚¬{df_quotes[price_col].std():,.0f}\")\n",
    "        print(f\"   â€¢ Missing values: {df_quotes[price_col].isna().sum():,} ({df_quotes[price_col].isna().mean()*100:.1f}%)\")\n",
    "        \n",
    "        if 'fg_devis_accepte' in df_quotes.columns:\n",
    "            # Price analysis by conversion\n",
    "            print(f\"\\nðŸŽ¯ Price Analysis by Conversion Status:\")\n",
    "            \n",
    "            converted_prices = df_quotes[df_quotes['fg_devis_accepte'] == 1][price_col]\n",
    "            non_converted_prices = df_quotes[df_quotes['fg_devis_accepte'] == 0][price_col]\n",
    "            \n",
    "            print(f\"   â€¢ Converted quotes: â‚¬{converted_prices.mean():,.0f} (median: â‚¬{converted_prices.median():,.0f})\")\n",
    "            print(f\"   â€¢ Non-converted quotes: â‚¬{non_converted_prices.mean():,.0f} (median: â‚¬{non_converted_prices.median():,.0f})\")\n",
    "            \n",
    "            # Price quartile analysis\n",
    "            valid_prices = df_quotes[price_col].dropna()\n",
    "            if len(valid_prices) > 0:\n",
    "                df_quotes['price_quartile'] = pd.qcut(df_quotes[price_col], q=4, \n",
    "                                                     labels=['Q1 (lowest)', 'Q2', 'Q3', 'Q4 (highest)'], \n",
    "                                                     duplicates='drop')\n",
    "                quartile_conversion = df_quotes.groupby('price_quartile')['fg_devis_accepte'].mean() * 100\n",
    "                \n",
    "                print(f\"\\nðŸ“ˆ Conversion by Price Quartile:\")\n",
    "                for quartile, rate in quartile_conversion.items():\n",
    "                    print(f\"   â€¢ {quartile}: {rate:.1f}% conversion\")\n",
    "                \n",
    "                # Find optimal price range\n",
    "                best_quartile = quartile_conversion.idxmax()\n",
    "                best_rate = quartile_conversion.max()\n",
    "                print(f\"   âš¡ OPTIMAL: {best_quartile} has highest conversion at {best_rate:.1f}%\")\n",
    "                \n",
    "                analysis_results['price_quartile_conversion'] = quartile_conversion\n",
    "    else:\n",
    "        print(\"âŒ No price column found for price analysis\")\n",
    "    \n",
    "    # ========== 4. BRAND SIGNAL ANALYSIS ==========\n",
    "    print(\"\\n\\nðŸ·ï¸ 4. BRAND SIGNAL ANALYSIS\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    brand_cols = ['brand', 'marque', 'nom_marque', 'marque_produit', 'libelle_marque']\n",
    "    brand_col = None\n",
    "    \n",
    "    for col in brand_cols:\n",
    "        if col in df_quotes.columns and df_quotes[col].notna().sum() > 0:\n",
    "            brand_col = col\n",
    "            break\n",
    "    \n",
    "    if brand_col:\n",
    "        print(f\"âœ… Found brand column: '{brand_col}'\")\n",
    "        \n",
    "        # Clean brand names\n",
    "        df_quotes[brand_col] = df_quotes[brand_col].astype(str).str.strip().str.upper()\n",
    "        \n",
    "        # Brand distribution\n",
    "        brand_counts = df_quotes[brand_col].value_counts()\n",
    "        brand_pct = df_quotes[brand_col].value_counts(normalize=True) * 100\n",
    "        \n",
    "        print(f\"\\nðŸ† Top 10 Brands by Quote Volume:\")\n",
    "        top_brands = brand_counts.head(10)\n",
    "        \n",
    "        for i, (brand, count) in enumerate(top_brands.items(), 1):\n",
    "            pct = brand_pct[brand]\n",
    "            print(f\"   {i:2d}. {brand:25} {count:6,} quotes ({pct:.1f}% share)\")\n",
    "        \n",
    "        if 'fg_devis_accepte' in df_quotes.columns:\n",
    "            # Brand conversion analysis\n",
    "            print(f\"\\nðŸŽ¯ Top 10 Brands by Conversion Rate (min 50 quotes):\")\n",
    "            \n",
    "            # Filter brands with sufficient data\n",
    "            brand_stats = df_quotes.groupby(brand_col).agg(\n",
    "                quote_count=('fg_devis_accepte', 'count'),\n",
    "                conversion_rate=('fg_devis_accepte', 'mean')\n",
    "            )\n",
    "            \n",
    "            # Get top converting brands with enough quotes\n",
    "            qualifying_brands = brand_stats[brand_stats['quote_count'] >= 50]\n",
    "            \n",
    "            if len(qualifying_brands) > 0:\n",
    "                top_converting = qualifying_brands.nlargest(10, 'conversion_rate')\n",
    "                \n",
    "                for i, (brand, stats) in enumerate(top_converting.iterrows(), 1):\n",
    "                    conv_rate = stats['conversion_rate'] * 100\n",
    "                    count = int(stats['quote_count'])\n",
    "                    print(f\"   {i:2d}. {brand:25} {conv_rate:5.1f}% conversion ({count:,} quotes)\")\n",
    "                \n",
    "                # Low converting brands\n",
    "                print(f\"\\nâš ï¸  Bottom 5 Converting Brands (min 50 quotes):\")\n",
    "                low_converting = qualifying_brands.nsmallest(5, 'conversion_rate')\n",
    "                \n",
    "                for i, (brand, stats) in enumerate(low_converting.iterrows(), 1):\n",
    "                    conv_rate = stats['conversion_rate'] * 100\n",
    "                    count = int(stats['quote_count'])\n",
    "                    print(f\"   {i:2d}. {brand:25} {conv_rate:5.1f}% conversion ({count:,} quotes)\")\n",
    "                \n",
    "                analysis_results['brand_conversion_stats'] = qualifying_brands\n",
    "            else:\n",
    "                print(\"   No brands have sufficient data for conversion analysis\")\n",
    "    else:\n",
    "        print(\"âŒ No brand column found for brand analysis\")\n",
    "    \n",
    "    # ========== 5. EQUIPMENT SIGNAL ANALYSIS ==========\n",
    "    print(\"\\n\\nâš™ï¸ 5. EQUIPMENT SIGNAL ANALYSIS\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    equipment_cols = ['regroup_famille_equipement_produit', 'famille_equipement_produit', \n",
    "                      'equipment_type', 'type_equipement', 'libelle_equipement']\n",
    "    equipment_col = None\n",
    "    \n",
    "    for col in equipment_cols:\n",
    "        if col in df_quotes.columns and df_quotes[col].notna().sum() > 0:\n",
    "            equipment_col = col\n",
    "            break\n",
    "    \n",
    "    if equipment_col:\n",
    "        print(f\"âœ… Found equipment column: '{equipment_col}'\")\n",
    "        \n",
    "        # Clean equipment names\n",
    "        df_quotes[equipment_col] = df_quotes[equipment_col].astype(str).str.strip()\n",
    "        \n",
    "        # Equipment distribution\n",
    "        equipment_counts = df_quotes[equipment_col].value_counts()\n",
    "        equipment_pct = df_quotes[equipment_col].value_counts(normalize=True) * 100\n",
    "        \n",
    "        print(f\"\\nðŸ”§ Top 10 Equipment Categories:\")\n",
    "        for i, (category, count) in enumerate(equipment_counts.head(10).items(), 1):\n",
    "            pct = equipment_pct[category]\n",
    "            print(f\"   {i:2d}. {category:35} {count:6,} quotes ({pct:.1f}%)\")\n",
    "        \n",
    "        if 'fg_devis_accepte' in df_quotes.columns:\n",
    "            # Equipment conversion analysis\n",
    "            print(f\"\\nðŸŽ¯ Equipment Conversion Analysis (min 50 quotes):\")\n",
    "            \n",
    "            equipment_stats = df_quotes.groupby(equipment_col).agg(\n",
    "                quote_count=('fg_devis_accepte', 'count'),\n",
    "                conversion_rate=('fg_devis_accepte', 'mean')\n",
    "            )\n",
    "            \n",
    "            # Get top converting equipment categories\n",
    "            qualifying_equipment = equipment_stats[equipment_stats['quote_count'] >= 50]\n",
    "            \n",
    "            if len(qualifying_equipment) > 0:\n",
    "                top_equipment = qualifying_equipment.nlargest(8, 'conversion_rate')\n",
    "                \n",
    "                for i, (category, stats) in enumerate(top_equipment.iterrows(), 1):\n",
    "                    conv_rate = stats['conversion_rate'] * 100\n",
    "                    count = int(stats['quote_count'])\n",
    "                    print(f\"   {i:2d}. {category:35} {conv_rate:5.1f}% conversion ({count:,} quotes)\")\n",
    "                \n",
    "                analysis_results['equipment_conversion_stats'] = equipment_stats\n",
    "            else:\n",
    "                print(\"   No equipment categories have sufficient data for conversion analysis\")\n",
    "    else:\n",
    "        print(\"âŒ No equipment column found for equipment analysis\")\n",
    "    \n",
    "    # ========== 6. PROCESS ADOPTION ANALYSIS ==========\n",
    "    print(\"\\n\\nðŸ”„ 6. PROCESS ADOPTION ANALYSIS\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    process_cols = ['fg_nouveau_process_relance_devis', 'nouveau_process', 'process_type']\n",
    "    process_col = None\n",
    "    \n",
    "    for col in process_cols:\n",
    "        if col in df_quotes.columns:\n",
    "            process_col = col\n",
    "            break\n",
    "    \n",
    "    if process_col:\n",
    "        print(f\"âœ… Found process column: '{process_col}'\")\n",
    "        \n",
    "        process_dist = df_quotes[process_col].value_counts(normalize=True) * 100\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Process Distribution:\")\n",
    "        total_quotes = len(df_quotes)\n",
    "        for process, count in df_quotes[process_col].value_counts().items():\n",
    "            pct = count / total_quotes * 100\n",
    "            if pd.isna(process):\n",
    "                print(f\"   â€¢ Missing/Unknown: {count:,} quotes ({pct:.1f}%)\")\n",
    "            else:\n",
    "                process_name = \"New Process\" if process == 1 else \"Old Process\"\n",
    "                print(f\"   â€¢ {process_name}: {count:,} quotes ({pct:.1f}%)\")\n",
    "        \n",
    "        if 'fg_devis_accepte' in df_quotes.columns:\n",
    "            print(f\"\\nðŸŽ¯ Process Conversion Analysis:\")\n",
    "            process_conversion = df_quotes.groupby(process_col)['fg_devis_accepte'].mean() * 100\n",
    "            \n",
    "            for process, conv_rate in process_conversion.items():\n",
    "                if pd.isna(process):\n",
    "                    process_name = \"Missing/Unknown\"\n",
    "                else:\n",
    "                    process_name = \"New Process\" if process == 1 else \"Old Process\"\n",
    "                print(f\"   â€¢ {process_name}: {conv_rate:.1f}% conversion\")\n",
    "            \n",
    "            # Calculate improvement\n",
    "            if 1 in process_conversion.index and 0 in process_conversion.index:\n",
    "                new_rate = process_conversion[1]\n",
    "                old_rate = process_conversion[0]\n",
    "                improvement = new_rate - old_rate\n",
    "                print(f\"   âš¡ IMPACT: New process shows {improvement:+.1f}% conversion change\")\n",
    "            \n",
    "            analysis_results['process_conversion'] = process_conversion\n",
    "    else:\n",
    "        print(\"âŒ No process column found for process analysis\")\n",
    "    \n",
    "    # ========== 7. COMMERCIAL ROLE ANALYSIS ==========\n",
    "    print(\"\\n\\nðŸ‘¥ 7. COMMERCIAL ROLE ANALYSIS\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    role_cols = ['fonction_commercial', 'commercial_role', 'role_commercial']\n",
    "    role_col = None\n",
    "    \n",
    "    for col in role_cols:\n",
    "        if col in df_quotes.columns:\n",
    "            role_col = col\n",
    "            break\n",
    "    \n",
    "    if role_col:\n",
    "        print(f\"âœ… Found commercial role column: '{role_col}'\")\n",
    "        \n",
    "        role_counts = df_quotes[role_col].value_counts()\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Top 10 Commercial Roles:\")\n",
    "        for i, (role, count) in enumerate(role_counts.head(10).items(), 1):\n",
    "            pct = count / len(df_quotes) * 100\n",
    "            print(f\"   {i:2d}. {role:30} {count:6,} quotes ({pct:.1f}%)\")\n",
    "        \n",
    "        if 'fg_devis_accepte' in df_quotes.columns:\n",
    "            print(f\"\\nðŸŽ¯ Top Converting Roles (min 30 quotes):\")\n",
    "            \n",
    "            role_stats = df_quotes.groupby(role_col).agg(\n",
    "                quote_count=('fg_devis_accepte', 'count'),\n",
    "                conversion_rate=('fg_devis_accepte', 'mean')\n",
    "            )\n",
    "            \n",
    "            # Filter for significant roles\n",
    "            significant_roles = role_stats[role_stats['quote_count'] >= 30]\n",
    "            \n",
    "            if len(significant_roles) > 0:\n",
    "                top_roles = significant_roles.nlargest(10, 'conversion_rate')\n",
    "                \n",
    "                for i, (role, stats) in enumerate(top_roles.iterrows(), 1):\n",
    "                    conv_rate = stats['conversion_rate'] * 100\n",
    "                    count = int(stats['quote_count'])\n",
    "                    print(f\"   {i:2d}. {role:30} {conv_rate:5.1f}% conversion ({count:,} quotes)\")\n",
    "                \n",
    "                analysis_results['role_conversion_stats'] = significant_roles\n",
    "            else:\n",
    "                print(\"   No roles have sufficient data for conversion analysis\")\n",
    "    else:\n",
    "        print(\"âŒ No commercial role column found\")\n",
    "    \n",
    "    # ========== 8. AGENCY PERFORMANCE ANALYSIS ==========\n",
    "    print(\"\\n\\nðŸ¢ 8. AGENCY PERFORMANCE ANALYSIS\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    agency_cols = ['code_agence', 'agence', 'nom_agence', 'libelle_agence']\n",
    "    agency_col = None\n",
    "    \n",
    "    for col in agency_cols:\n",
    "        if col in df_quotes.columns:\n",
    "            agency_col = col\n",
    "            break\n",
    "    \n",
    "    if agency_col:\n",
    "        print(f\"âœ… Found agency column: '{agency_col}'\")\n",
    "        \n",
    "        agency_counts = df_quotes[agency_col].value_counts()\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Top 10 Agencies by Quote Volume:\")\n",
    "        for i, (agency, count) in enumerate(agency_counts.head(10).items(), 1):\n",
    "            pct = count / len(df_quotes) * 100\n",
    "            print(f\"   {i:2d}. {agency:25} {count:6,} quotes ({pct:.1f}%)\")\n",
    "        \n",
    "        if 'fg_devis_accepte' in df_quotes.columns:\n",
    "            print(f\"\\nðŸŽ¯ Agency Conversion Performance (min 100 quotes):\")\n",
    "            \n",
    "            agency_stats = df_quotes.groupby(agency_col).agg(\n",
    "                quote_count=('fg_devis_accepte', 'count'),\n",
    "                conversion_rate=('fg_devis_accepte', 'mean')\n",
    "            )\n",
    "            \n",
    "            if price_col:\n",
    "                agency_stats['avg_price'] = df_quotes.groupby(agency_col)[price_col].mean()\n",
    "            \n",
    "            # Filter for significant agencies\n",
    "            significant_agencies = agency_stats[agency_stats['quote_count'] >= 100]\n",
    "            \n",
    "            if len(significant_agencies) > 0:\n",
    "                print(f\"\\nðŸ† Top 5 Converting Agencies:\")\n",
    "                top_agencies = significant_agencies.nlargest(5, 'conversion_rate')\n",
    "                for i, (agency, stats) in enumerate(top_agencies.iterrows(), 1):\n",
    "                    conv_rate = stats['conversion_rate'] * 100\n",
    "                    count = int(stats['quote_count'])\n",
    "                    avg_price = f\"â‚¬{stats['avg_price']:,.0f}\" if price_col and 'avg_price' in stats else \"N/A\"\n",
    "                    print(f\"   {i:2d}. {agency:20} {conv_rate:5.1f}% conversion, {count:5,} quotes, avg: {avg_price}\")\n",
    "                \n",
    "                print(f\"\\nâš ï¸  Bottom 5 Converting Agencies:\")\n",
    "                bottom_agencies = significant_agencies.nsmallest(5, 'conversion_rate')\n",
    "                for i, (agency, stats) in enumerate(bottom_agencies.iterrows(), 1):\n",
    "                    conv_rate = stats['conversion_rate'] * 100\n",
    "                    count = int(stats['quote_count'])\n",
    "                    avg_price = f\"â‚¬{stats['avg_price']:,.0f}\" if price_col and 'avg_price' in stats else \"N/A\"\n",
    "                    print(f\"   {i:2d}. {agency:20} {conv_rate:5.1f}% conversion, {count:5,} quotes, avg: {avg_price}\")\n",
    "                \n",
    "                analysis_results['agency_stats'] = significant_agencies\n",
    "            else:\n",
    "                print(\"   No agencies have sufficient data for conversion analysis\")\n",
    "    else:\n",
    "        print(\"âŒ No agency column found for agency analysis\")\n",
    "    \n",
    "    # ========== 9. FEATURE DATASET ANALYSIS (X_new_clean) ==========\n",
    "    if X_new_clean is not None:\n",
    "        print(\"\\n\\nðŸ“Š 9. FEATURE DATASET ANALYSIS (X_new_clean)\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        print(f\"Feature dataset shape: {X_new_clean.shape}\")\n",
    "        print(f\"Feature types distribution:\")\n",
    "        \n",
    "        # Count feature types\n",
    "        numeric_features = X_new_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        categorical_features = X_new_clean.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        bool_features = X_new_clean.select_dtypes(include=['bool']).columns.tolist()\n",
    "        \n",
    "        print(f\"   â€¢ Numeric features: {len(numeric_features)}\")\n",
    "        print(f\"   â€¢ Categorical features: {len(categorical_features)}\")\n",
    "        print(f\"   â€¢ Boolean features: {len(bool_features)}\")\n",
    "        \n",
    "        # Show top features by variance (for numeric)\n",
    "        if len(numeric_features) > 0:\n",
    "            print(f\"\\nðŸ“ˆ Top 10 Numeric Features by Variance:\")\n",
    "            variances = X_new_clean[numeric_features].var().sort_values(ascending=False)\n",
    "            for i, (feature, variance) in enumerate(variances.head(10).items(), 1):\n",
    "                print(f\"   {i:2d}. {feature:35} variance: {variance:.4f}\")\n",
    "        \n",
    "        # Show top categorical features by unique values\n",
    "        if len(categorical_features) > 0:\n",
    "            print(f\"\\nðŸ·ï¸  Categorical Features Cardinality:\")\n",
    "            unique_counts = X_new_clean[categorical_features].nunique().sort_values(ascending=False)\n",
    "            for i, (feature, count) in enumerate(unique_counts.head(10).items(), 1):\n",
    "                print(f\"   {i:2d}. {feature:35} unique values: {count}\")\n",
    "    \n",
    "    # ========== 10. SUMMARY & INSIGHTS ==========\n",
    "    print(\"\\n\\nðŸ“‹ 10. DATA QUALITY & INSIGHTS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Data quality metrics\n",
    "    print(f\"\\nðŸ“Š Dataset Overview:\")\n",
    "    print(f\"   â€¢ Total quotes in df_quotes: {len(df_quotes):,}\")\n",
    "    print(f\"   â€¢ Unique customers: {df_quotes[customer_id_col].nunique():,}\")\n",
    "    \n",
    "    if 'fg_devis_accepte' in df_quotes.columns:\n",
    "        conversion_rate = df_quotes['fg_devis_accepte'].mean() * 100\n",
    "        print(f\"   â€¢ Overall conversion rate: {conversion_rate:.1f}%\")\n",
    "    \n",
    "    # Missing data analysis\n",
    "    print(f\"\\nðŸ” Missing Data Analysis (df_quotes):\")\n",
    "    missing_data = df_quotes.isnull().sum()\n",
    "    missing_pct = (missing_data / len(df_quotes)) * 100\n",
    "    \n",
    "    # Show top columns with missing data\n",
    "    missing_summary = pd.DataFrame({\n",
    "        'missing_count': missing_data,\n",
    "        'missing_percent': missing_pct\n",
    "    }).sort_values('missing_percent', ascending=False)\n",
    "    \n",
    "    print(\"   Top columns with missing values:\")\n",
    "    for i, (col, row) in enumerate(missing_summary.head(10).iterrows(), 1):\n",
    "        if row['missing_percent'] > 0:\n",
    "            print(f\"   {i:2d}. {col:35} {row['missing_count']:,} missing ({row['missing_percent']:.1f}%)\")\n",
    "    \n",
    "    # Generate insights\n",
    "    print(f\"\\nðŸ’¡ KEY INSIGHTS:\")\n",
    "    \n",
    "    insights = []\n",
    "    \n",
    "    # Insight 1: Single-quote conversion advantage\n",
    "    if 'single_vs_multi_conversion' in analysis_results:\n",
    "        single_conv = analysis_results['single_vs_multi_conversion']['single']\n",
    "        multi_conv = analysis_results['single_vs_multi_conversion']['multi']\n",
    "        diff = analysis_results['single_vs_multi_conversion']['difference']\n",
    "        \n",
    "        if single_conv > multi_conv:\n",
    "            insights.append(f\"â€¢ Single-quote customers convert {diff:.1f}% HIGHER than multi-quote customers\")\n",
    "        else:\n",
    "            insights.append(f\"â€¢ Multi-quote customers convert {diff:.1f}% HIGHER than single-quote customers\")\n",
    "    \n",
    "    # Insight 2: Price-conversion relationship\n",
    "    if 'price_quartile_conversion' in analysis_results:\n",
    "        quartiles = analysis_results['price_quartile_conversion']\n",
    "        if len(quartiles) >= 2:\n",
    "            best_q = quartiles.idxmax()\n",
    "            worst_q = quartiles.idxmin()\n",
    "            best_rate = quartiles.max()\n",
    "            worst_rate = quartiles.min()\n",
    "            \n",
    "            insights.append(f\"â€¢ {best_q} quotes convert {best_rate-worst_rate:.1f}% better than {worst_q} quotes\")\n",
    "    \n",
    "    # Insight 3: Process effectiveness\n",
    "    if 'process_conversion' in analysis_results:\n",
    "        process_rates = analysis_results['process_conversion']\n",
    "        if 1 in process_rates.index and 0 in process_rates.index:\n",
    "            new_rate = process_rates[1]\n",
    "            old_rate = process_rates[0]\n",
    "            if not pd.isna(new_rate) and not pd.isna(old_rate):\n",
    "                improvement = new_rate - old_rate\n",
    "                if improvement > 0:\n",
    "                    insights.append(f\"â€¢ New process converts {improvement:.1f}% BETTER than old process\")\n",
    "                elif improvement < 0:\n",
    "                    insights.append(f\"â€¢ New process converts {abs(improvement):.1f}% WORSE than old process\")\n",
    "    \n",
    "    # Print insights\n",
    "    if insights:\n",
    "        for i, insight in enumerate(insights, 1):\n",
    "            print(f\"   {i}. {insight}\")\n",
    "    else:\n",
    "        print(\"   No clear predictive patterns identified from basic analysis\")\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "# ========== MAIN EXECUTION ==========\n",
    "if __name__ == \"__main__\":\n",
    "    # Run enhanced analysis using your datasets\n",
    "    print(\"ðŸš€ STARTING ENHANCED DATA DISCOVERY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Run the main analysis\n",
    "    analysis_results = enhanced_data_discovery_analysis(\n",
    "        df_quotes=df_quotes,  # Your raw dataset with customer IDs\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ANALYSIS COMPLETE - READY FOR FEATURE ENGINEERING OPTIMIZATION\")\n",
    "    print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[Poetry] ai-france-hvac",
   "language": "python",
   "name": "ai-france-hvac-poetry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
