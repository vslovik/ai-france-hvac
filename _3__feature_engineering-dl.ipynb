{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc0afe3e-bd35-448e-b1bb-e76eef9a9ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Original quote data: 34,014 quotes from 23,888 customers\n",
      "\n",
      "================================================================================\n",
      "STRATEGY: CREATE MEANINGFUL SEQUENCE FEATURES\n",
      "================================================================================\n",
      "\n",
      "üìä Original quote data: 34,014 quotes from 23,888 customers\n",
      "\n",
      "================================================================================\n",
      "STRATEGY: CREATE MEANINGFUL SEQUENCE FEATURES\n",
      "================================================================================\n",
      "Creating enhanced customer features...\n",
      "  Total customers: 23,888\n",
      "‚úì Created features for 23,888 customers\n",
      "‚úì New features: ['numero_compte', 'total_quotes', 'converted', 'avg_days_between_quotes', 'std_days_between_quotes', 'max_days_between_quotes', 'engagement_density', 'price_trajectory', 'unique_product_families', 'product_consistency']...\n",
      "Creating sequence features (this may take a moment)...\n",
      "  Total customers: 23,888\n",
      "‚úì Created features for 23,888 customers\n",
      "‚úì New features: ['numero_compte', 'total_quotes', 'converted', 'avg_days_since_first_quote', 'std_days_since_first_quote', 'max_days_since_first_quote', 'avg_recent_quote_count', 'std_recent_quote_count', 'avg_recent_avg_price', 'std_recent_avg_price']...\n",
      "Index(['numero_compte', 'total_quotes', 'converted',\n",
      "       'avg_days_since_first_quote', 'std_days_since_first_quote',\n",
      "       'max_days_since_first_quote', 'avg_recent_quote_count',\n",
      "       'std_recent_quote_count', 'avg_recent_avg_price',\n",
      "       'std_recent_avg_price', 'avg_recent_price_std', 'std_recent_price_std',\n",
      "       'avg_recent_product_variety', 'std_recent_product_variety',\n",
      "       'avg_recent_conversion_rate', 'std_recent_conversion_rate',\n",
      "       'avg_current_price', 'std_current_price', 'sequence_quote_ratio',\n",
      "       'price_trend', 'conversion_rate_trend'],\n",
      "      dtype='object')\n",
      "Columns: ['numero_compte', 'total_quotes_seq', 'converted_seq', 'avg_days_since_first_quote', 'std_days_since_first_quote', 'max_days_since_first_quote', 'avg_recent_quote_count', 'std_recent_quote_count', 'avg_recent_avg_price', 'std_recent_avg_price', 'avg_recent_price_std', 'std_recent_price_std', 'avg_recent_product_variety', 'std_recent_product_variety', 'avg_recent_conversion_rate', 'std_recent_conversion_rate', 'avg_current_price', 'std_current_price', 'sequence_quote_ratio', 'price_trend', 'conversion_rate_trend', 'total_quotes', 'converted', 'avg_days_between_quotes', 'std_days_between_quotes', 'max_days_between_quotes', 'engagement_density', 'price_trajectory', 'unique_product_families', 'product_consistency', 'avg_price', 'price_range', 'price_volatility', 'main_agency', 'main_region', 'avg_discount_pct']\n",
      "\n",
      "üîß ENCODING & PREPARING FOR MODELING...\n",
      "  Preparing Customer Features...\n",
      "  Features: 14, Samples: 23888\n",
      "\n",
      "üîß ENCODING & PREPARING FOR MODELING...\n",
      "  Preparing Sequence Features...\n",
      "  Features: 32, Samples: 23888\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from etl.util import prepare_dataset_without_leakage\n",
    "from ml_features.features import prepare_features\n",
    "from ml_features.customer_features import create_customer_features\n",
    "from ml_features.sequence_features  import create_sequence_features\n",
    "from ml_training.train_rf import train_rf\n",
    "from ml_evaluation.dashboard import model_evaluation_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load original clean quote data\n",
    "df_quotes = pd.read_csv('cleaned_quote_data.csv')\n",
    "df_quotes['dt_creation_devis'] = pd.to_datetime(df_quotes['dt_creation_devis'])\n",
    "\n",
    "print(f\"\\nüìä Original quote data: {len(df_quotes):,} quotes from {df_quotes['numero_compte'].nunique():,} customers\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STRATEGY: CREATE MEANINGFUL SEQUENCE FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from etl.util import prepare_dataset_without_leakage\n",
    "from ml_features.features import prepare_features\n",
    "from ml_features.customer_features import create_customer_features\n",
    "from ml_features.sequence_features  import create_sequence_features\n",
    "from ml_training.train_rf import train_rf\n",
    "from ml_evaluation.dashboard import model_evaluation_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load original clean quote data\n",
    "df_quotes = pd.read_csv('cleaned_quote_data.csv')\n",
    "df_quotes['dt_creation_devis'] = pd.to_datetime(df_quotes['dt_creation_devis'])\n",
    "\n",
    "print(f\"\\nüìä Original quote data: {len(df_quotes):,} quotes from {df_quotes['numero_compte'].nunique():,} customers\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STRATEGY: CREATE MEANINGFUL SEQUENCE FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Enhanced customer features\n",
    "customer_df = create_customer_features(df_quotes)\n",
    "len(customer_df)\n",
    "\n",
    "# 2. Sequence features (for multi-quote customers)\n",
    "sequence_df = create_sequence_features(df_quotes)\n",
    "print(sequence_df.columns)\n",
    "\n",
    "sequence_df = pd.merge(\n",
    "    sequence_df, \n",
    "    customer_df, \n",
    "    on='numero_compte', \n",
    "    how='left',\n",
    "    suffixes=('_seq', '')  # Explicit suffixes\n",
    ")\n",
    "\n",
    "print(\"Columns:\", sequence_df.columns.tolist())\n",
    "\n",
    "# Now it's clear which column is which\n",
    "y_sequence = sequence_df['converted']  # From sequence features\n",
    "y_customer = sequence_df['converted']  # From customer features\n",
    "\n",
    "# For modeling, use the sequence version\n",
    "X_customer = customer_df.drop(columns=['numero_compte', 'converted'], errors='ignore')\n",
    "X_customer_clean, y_customer_clean = prepare_features(X_customer, y_customer, \"Customer Features\")\n",
    "\n",
    "columns_to_drop =  [x for x in sequence_df.columns if '_seq' in x]\n",
    "columns_to_drop.extend(['numero_compte', 'converted'])\n",
    "X_sequence = sequence_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "X_sequence_clean, y_sequence_clean = prepare_features(X_sequence, y_sequence, \"Sequence Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13f24c30-4793-4902-969d-0d59aff557d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CREATING SAFE DL-OPTIMIZED FEATURES (V2)\n",
      "================================================================================\n",
      "üìä Input shape: (23888, 32)\n",
      "üìã Found 32 numeric columns\n",
      "\n",
      "üîß Step 1: Scaling features to reasonable range...\n",
      "\n",
      "üîß Step 2: Adding safe transformations to ALL numeric features...\n",
      "\n",
      "üîß Step 3: Adding safe interactions...\n",
      "    ‚úì Added interaction: avg_days_since_first_quote / std_days_since_first_quote\n",
      "\n",
      "üîß Step 4: Clipping all features to safe range...\n",
      "\n",
      "‚úÖ SAFE DL Features Created:\n",
      "  Original: 32 features\n",
      "  Final: 129 features\n",
      "  Added: 97 new features\n",
      "\n",
      "üìä Safe value ranges:\n",
      "  Min: -10.00\n",
      "  Max: 10.00\n",
      "  Mean: 0.34\n"
     ]
    }
   ],
   "source": [
    "from dl_training.train import train_advanced_dl_model\n",
    "from dl_features.features import create_dl_specific_features\n",
    "\n",
    "X_dl_optimized, y_dl = create_dl_specific_features(X_sequence_clean, y_sequence_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8750bce-a75b-4b61-8fa3-ab066e879631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 8 focused features\n"
     ]
    }
   ],
   "source": [
    "def create_focused_features(X, y):\n",
    "    \"\"\"Create new features focused on what matters most\"\"\"\n",
    "    X_focused = X.copy()\n",
    "    \n",
    "    # Double down on conversion rate features\n",
    "    if 'avg_recent_conversion_rate' in X.columns:\n",
    "        # More transformations of the most important feature\n",
    "        X_focused['conversion_rate_exp'] = np.exp(X['avg_recent_conversion_rate'].clip(-10, 10))\n",
    "        X_focused['conversion_rate_power3'] = X['avg_recent_conversion_rate'] ** 3\n",
    "        X_focused['conversion_rate_sigmoid'] = 1 / (1 + np.exp(-X['avg_recent_conversion_rate']))\n",
    "    \n",
    "    # Agency-conversion interactions\n",
    "    if 'main_agency_log' in X.columns and 'avg_recent_conversion_rate' in X.columns:\n",
    "        X_focused['agency_conversion_interaction'] = X['main_agency_log'] * X['avg_recent_conversion_rate']\n",
    "    \n",
    "    # Discount-conversion interactions  \n",
    "    if 'avg_discount_pct_abs_sqrt' in X.columns and 'avg_recent_conversion_rate' in X.columns:\n",
    "        X_focused['discount_conversion_interaction'] = X['avg_discount_pct_abs_sqrt'] * X['avg_recent_conversion_rate']\n",
    "    \n",
    "    # Price-conversion interactions\n",
    "    price_cols = [c for c in X.columns if 'price' in c.lower() and 'conversion' not in c.lower()]\n",
    "    if price_cols and 'avg_recent_conversion_rate' in X.columns:\n",
    "        for price_col in price_cols[:3]:  # Top 3 price features\n",
    "            X_focused[f'{price_col}_conversion_interaction'] = X[price_col] * X['avg_recent_conversion_rate']\n",
    "    \n",
    "    print(f\"Added {X_focused.shape[1] - X.shape[1]} focused features\")\n",
    "    return X_focused\n",
    "\n",
    "# Create focused features\n",
    "X_focused = create_focused_features(X_dl_optimized, y_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36af9f88-80e0-4252-9805-00e31428bd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 3 region-focused features\n"
     ]
    }
   ],
   "source": [
    "def enhance_region_features(X):\n",
    "    \"\"\"Enhance region-related features since they're most important\"\"\"\n",
    "    X_enhanced = X.copy()\n",
    "    \n",
    "    if 'main_region' in X.columns:\n",
    "        # Region is categorical but encoded as numeric - create better features\n",
    "        # Create region clusters if you have more info\n",
    "        X_enhanced['region_is_popular'] = (X['main_region'] == X['main_region'].mode()[0]).astype(int)\n",
    "        \n",
    "        # Region interactions with price\n",
    "        if 'avg_current_price' in X.columns:\n",
    "            X_enhanced['region_price_interaction'] = X['main_region'] * X['avg_current_price']\n",
    "        \n",
    "        # Region interactions with discount\n",
    "        if 'avg_discount_pct' in X.columns:\n",
    "            X_enhanced['region_discount_interaction'] = X['main_region'] * X['avg_discount_pct']\n",
    "    \n",
    "    print(f\"Added {X_enhanced.shape[1] - X.shape[1]} region-focused features\")\n",
    "    return X_enhanced\n",
    "\n",
    "X_region_enhanced = enhance_region_features(X_focused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d74b509-856c-4443-8e0a-76bd29937a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 0 region-focused features\n"
     ]
    }
   ],
   "source": [
    "def enhance_discount_features(X):\n",
    "    \"\"\"Discounts are #2 important - enhance them\"\"\"\n",
    "    X_enhanced = X.copy()\n",
    "    \n",
    "    if 'avg_discount_pct' in X.columns:\n",
    "        # Discount tiers\n",
    "        X_enhanced['discount_tier'] = pd.cut(\n",
    "            X['avg_discount_pct'], \n",
    "            bins=[-np.inf, 0, 5, 10, 20, np.inf],\n",
    "            labels=['negative', 'small', 'medium', 'large', 'very_large']\n",
    "        ).cat.codes\n",
    "        \n",
    "        # Is there any discount?\n",
    "        X_enhanced['has_discount'] = (X['avg_discount_pct'] > 0).astype(int)\n",
    "        \n",
    "        # Discount effectiveness (interact with price)\n",
    "        if 'avg_current_price' in X.columns:\n",
    "            X_enhanced['discount_price_ratio'] = X['avg_discount_pct'] / (X['avg_current_price'] + 1)\n",
    "    \n",
    "    print(f\"Added {X_enhanced.shape[1] - X.shape[1]} discount-focused features\")\n",
    "    return X_enhanced\n",
    "\n",
    "X_discount_enhanced = enhance_region_features(X_region_enhanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f627bf8b-6445-425e-ac05-f76404378a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Training ADVANCED DL Model...\n",
      "üîß Normalizing features for DL...\n",
      "  Before: min=-20.00, max=20.00\n",
      "  Before: mean=0.33, std=1.47\n",
      "  After: min=-5.43, max=6.06\n",
      "  After: mean=0.65, std=1.53\n",
      "  Normalization complete!\n",
      "üîß Normalizing features for DL...\n",
      "  Before: min=-20.00, max=13.41\n",
      "  Before: mean=0.33, std=1.48\n",
      "  After: min=-1.12, max=4.58\n",
      "  After: mean=1.25, std=1.59\n",
      "  Normalization complete!\n",
      "  Parameters: 96,333\n",
      "  Model: advanced\n",
      "  Input dim: 140\n",
      "  Parameters: 96,333\n",
      "  Training samples: 19,110\n",
      "  Validation samples: 4,778\n",
      "  ‚úì Epoch 1: Loss=0.8667, Val AUC=0.6608\n",
      "  ‚úì Epoch 2: Loss=0.8149, Val AUC=0.6718\n",
      "  ‚úì Epoch 3: Loss=0.7968, Val AUC=0.6743\n",
      "  ‚úì Epoch 4: Loss=0.7900, Val AUC=0.6784\n",
      "  ‚úì Epoch 5: Loss=0.7834, Val AUC=0.6800\n",
      "  ‚úì Epoch 6: Loss=0.7808, Val AUC=0.6802\n",
      "  ‚úì Epoch 7: Loss=0.7792, Val AUC=0.6807\n",
      "  ‚úì Epoch 9: Loss=0.7735, Val AUC=0.6808\n",
      "  ‚úì Epoch 10: Loss=0.7725, Val AUC=0.6842\n",
      "  ‚úì Epoch 19: Loss=0.7620, Val AUC=0.6872\n",
      "  ‚èπÔ∏è Early stopping at epoch 44\n",
      "\n",
      "‚úÖ Training Complete!\n",
      "  Best Val AUC: 0.6872\n"
     ]
    }
   ],
   "source": [
    "model, auc = train_advanced_dl_model(\n",
    "    X_discount_enhanced, y_dl \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dafe5e90-a78e-43e4-84bc-6838621ea9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GRADIENT-BASED INPUT FEATURE IMPORTANCE\n",
      "================================================================================\n",
      "\n",
      "Top 20 input features by gradient magnitude:\n",
      "  region_discount_interaction              | Gradient: 0.206889\n",
      "  avg_price                                | Gradient: 0.169857\n",
      "  avg_discount_pct_abs_sqrt                | Gradient: 0.169613\n",
      "  avg_recent_conversion_rate_tanh          | Gradient: 0.157406\n",
      "  avg_current_price                        | Gradient: 0.127763\n",
      "  avg_days_between_quotes_log              | Gradient: 0.122048\n",
      "  avg_discount_pct_tanh                    | Gradient: 0.112039\n",
      "  avg_discount_pct_log                     | Gradient: 0.101935\n",
      "  std_days_since_first_quote_log           | Gradient: 0.098437\n",
      "  main_region_tanh                         | Gradient: 0.098250\n",
      "  std_recent_conversion_rate_abs_sqrt      | Gradient: 0.098126\n",
      "  region_price_interaction                 | Gradient: 0.096191\n",
      "  std_recent_avg_price                     | Gradient: 0.095091\n",
      "  main_region_log                          | Gradient: 0.090296\n",
      "  avg_price_abs_sqrt                       | Gradient: 0.084555\n",
      "  avg_days_since_first_quote_abs_sqrt      | Gradient: 0.074380\n",
      "  avg_current_price_log                    | Gradient: 0.072852\n",
      "  avg_recent_conversion_rate               | Gradient: 0.068014\n",
      "  avg_days_between_quotes                  | Gradient: 0.067261\n",
      "  std_recent_conversion_rate               | Gradient: 0.066879\n",
      "\n",
      "üîç FEATURE CATEGORY ANALYSIS:\n",
      "  Price     : 48 features | Avg importance: 0.041898\n",
      "  Quote     : 41 features | Avg importance: 0.038585\n",
      "  Day       : 25 features | Avg importance: 0.042333\n",
      "  Average   : 48 features | Avg importance: 0.053901\n",
      "  Std Dev   : 38 features | Avg importance: 0.037882\n",
      "  Trend     :  8 features | Avg importance: 0.038196\n",
      "  Ratio     :  5 features | Avg importance: 0.038907\n",
      "  Log       : 32 features | Avg importance: 0.044172\n",
      "  Tanh      : 32 features | Avg importance: 0.040058\n",
      "  Sqrt      : 32 features | Avg importance: 0.041578\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def analyze_attention_weights(model, X_sample):\n",
    "    \"\"\"\n",
    "    Analyze which HIDDEN features the attention mechanism focuses on\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a sample batch\n",
    "    if isinstance(X_sample, pd.DataFrame):\n",
    "        X_tensor = torch.FloatTensor(X_sample.values[:100])  # First 100 samples\n",
    "    else:\n",
    "        X_tensor = torch.FloatTensor(X_sample[:100])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Forward pass through network\n",
    "        features = model.net(X_tensor)  # Shape: [100, 64] (hidden features)\n",
    "        attention_weights = model.attention(features)  # Shape: [100, 64]\n",
    "        \n",
    "        # Get average attention per HIDDEN feature\n",
    "        avg_attention = attention_weights.mean(dim=0).squeeze().numpy()\n",
    "    \n",
    "    print(f\"Input features: {X_sample.shape[1]}\")\n",
    "    print(f\"Hidden features: {features.shape[1]}\")\n",
    "    \n",
    "    # Create importance DataFrame for HIDDEN features\n",
    "    importance_df = pd.DataFrame({\n",
    "        'hidden_feature_idx': list(range(len(avg_attention))),  # FIX: list of ints\n",
    "        'attention_weight': avg_attention\n",
    "    }).sort_values('attention_weight', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ATTENTION-BASED HIDDEN FEATURE IMPORTANCE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTop 20 hidden features by attention weight:\")\n",
    "    for i, row in importance_df.head(20).iterrows():\n",
    "        # FIX: Convert to int for formatting\n",
    "        feat_idx = int(row['hidden_feature_idx'])\n",
    "        print(f\"  Hidden feature {feat_idx:3d} | Attention: {row['attention_weight']:.4f}\")\n",
    "    \n",
    "    # Check if attention is actually working\n",
    "    variance = importance_df['attention_weight'].var()\n",
    "    print(f\"\\nüîç Attention variance: {variance:.6f}\")\n",
    "    if variance < 0.001:\n",
    "        print(\"‚ö†Ô∏è  WARNING: Attention weights are nearly identical!\")\n",
    "    else:\n",
    "        print(\"‚úÖ GOOD: Attention weights vary across features\")\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# BETTER: Analyze which INPUT features matter using gradients\n",
    "def analyze_input_feature_importance(model, X_sample):\n",
    "    \"\"\"\n",
    "    Analyze which INPUT features matter using gradient-based importance\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert to tensor with gradient tracking\n",
    "    X_tensor = torch.FloatTensor(X_sample.values[:100])\n",
    "    X_tensor.requires_grad = True\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(X_tensor)\n",
    "    \n",
    "    # Create dummy target for gradient computation\n",
    "    dummy_target = torch.ones_like(output)\n",
    "    \n",
    "    # Backward pass to get gradients w.r.t inputs\n",
    "    model.zero_grad()\n",
    "    output.backward(dummy_target)\n",
    "    \n",
    "    # Get average absolute gradient per INPUT feature\n",
    "    gradients = X_tensor.grad.abs().mean(dim=0).numpy()\n",
    "    \n",
    "    # Create importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X_sample.columns.tolist(),\n",
    "        'gradient_importance': gradients\n",
    "    }).sort_values('gradient_importance', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GRADIENT-BASED INPUT FEATURE IMPORTANCE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTop 20 input features by gradient magnitude:\")\n",
    "    for i, row in importance_df.head(20).iterrows():\n",
    "        print(f\"  {row['feature']:40s} | Gradient: {row['gradient_importance']:.6f}\")\n",
    "    \n",
    "    # Also show feature categories\n",
    "    print(f\"\\nüîç FEATURE CATEGORY ANALYSIS:\")\n",
    "    \n",
    "    categories = {\n",
    "        'Price': ['price'],\n",
    "        'Quote': ['quote'],\n",
    "        'Day': ['day'],\n",
    "        'Average': ['avg_'],\n",
    "        'Std Dev': ['std_'],\n",
    "        'Trend': ['trend'],\n",
    "        'Ratio': ['ratio', 'div', 'per'],\n",
    "        'Log': ['log'],\n",
    "        'Squared': ['squared'],\n",
    "        'Tanh': ['tanh'],\n",
    "        'Sqrt': ['sqrt']\n",
    "    }\n",
    "    \n",
    "    for cat_name, keywords in categories.items():\n",
    "        cat_features = [f for f in importance_df['feature'] \n",
    "                       if any(kw in f.lower() for kw in keywords)]\n",
    "        \n",
    "        if cat_features:\n",
    "            cat_importance = importance_df[\n",
    "                importance_df['feature'].isin(cat_features)\n",
    "            ]['gradient_importance'].mean()\n",
    "            \n",
    "            print(f\"  {cat_name:10s}: {len(cat_features):2d} features | \"\n",
    "                  f\"Avg importance: {cat_importance:.6f}\")\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Usage\n",
    "importance_df = analyze_input_feature_importance(model, X_region_enhanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9610cdcc-cc3b-4453-a7a9-29d5b370319f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GRADIENT-BASED FEATURE IMPORTANCE\n",
      "================================================================================\n",
      "\n",
      "Top 20 features by gradient magnitude:\n",
      "  region_discount_interaction              | Gradient: 0.206889\n",
      "  avg_price                                | Gradient: 0.169857\n",
      "  avg_discount_pct_abs_sqrt                | Gradient: 0.169613\n",
      "  avg_recent_conversion_rate_tanh          | Gradient: 0.157406\n",
      "  avg_current_price                        | Gradient: 0.127763\n",
      "  avg_days_between_quotes_log              | Gradient: 0.122048\n",
      "  avg_discount_pct_tanh                    | Gradient: 0.112039\n",
      "  avg_discount_pct_log                     | Gradient: 0.101935\n",
      "  std_days_since_first_quote_log           | Gradient: 0.098437\n",
      "  main_region_tanh                         | Gradient: 0.098250\n",
      "  std_recent_conversion_rate_abs_sqrt      | Gradient: 0.098126\n",
      "  region_price_interaction                 | Gradient: 0.096191\n",
      "  std_recent_avg_price                     | Gradient: 0.095091\n",
      "  main_region_log                          | Gradient: 0.090296\n",
      "  avg_price_abs_sqrt                       | Gradient: 0.084555\n",
      "  avg_days_since_first_quote_abs_sqrt      | Gradient: 0.074380\n",
      "  avg_current_price_log                    | Gradient: 0.072852\n",
      "  avg_recent_conversion_rate               | Gradient: 0.068014\n",
      "  avg_days_between_quotes                  | Gradient: 0.067261\n",
      "  std_recent_conversion_rate               | Gradient: 0.066879\n"
     ]
    }
   ],
   "source": [
    "def gradient_based_importance(model, X_sample, y_sample):\n",
    "    \"\"\"\n",
    "    Compute feature importance using gradients (Integrated Gradients-like)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert to tensor\n",
    "    X_tensor = torch.FloatTensor(X_sample.values[:100])\n",
    "    X_tensor.requires_grad = True\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(X_tensor)\n",
    "    \n",
    "    # Create dummy target (we want gradients w.r.t inputs)\n",
    "    dummy_target = torch.ones_like(output)\n",
    "    \n",
    "    # Backward pass to get gradients\n",
    "    model.zero_grad()\n",
    "    output.backward(dummy_target)\n",
    "    \n",
    "    # Get average absolute gradient per feature\n",
    "    gradients = X_tensor.grad.abs().mean(dim=0).numpy()\n",
    "    \n",
    "    # Create importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X_sample.columns.tolist(),\n",
    "        'gradient_importance': gradients\n",
    "    }).sort_values('gradient_importance', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GRADIENT-BASED FEATURE IMPORTANCE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTop 20 features by gradient magnitude:\")\n",
    "    for i, row in importance_df.head(20).iterrows():\n",
    "        print(f\"  {row['feature']:40s} | Gradient: {row['gradient_importance']:.6f}\")\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Usage\n",
    "grad_importance = gradient_based_importance(model, X_region_enhanced, y_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[Poetry] ai-france-hvac",
   "language": "python",
   "name": "ai-france-hvac-poetry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
