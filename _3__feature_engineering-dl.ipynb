{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc0afe3e-bd35-448e-b1bb-e76eef9a9ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Original quote data: 34,014 quotes from 23,888 customers\n",
      "\n",
      "================================================================================\n",
      "STRATEGY: CREATE FEATURES\n",
      "================================================================================\n",
      "Creating OPTIMIZED customer features (mode: first_conversion)...\n",
      "  Filtering post-first-purchase data...\n",
      "  Customers: 23,888, Quotes: 33,247\n",
      "  Calculating features...\n",
      "  Calculating price trajectory (optimized)...\n",
      "‚úì Created 14 leakage-free features\n",
      "‚Üí 23,888 customers | 39.6% converters\n",
      "‚è±Ô∏è  Execution time: 10.4 seconds\n",
      "‚ö†Ô∏è  10.4s (target was 3s)\n",
      "================================================================================\n",
      "CREATING FIRST CONVERSION PREDICTION FEATURES (LEAKAGE-FREE)\n",
      "================================================================================\n",
      "  Total customers: 23,888\n",
      "‚ö° Processing customers with corrected first-conversion logic...\n",
      "  Processed 0/23,888 customers\n",
      "  Processed 5,000/23,888 customers\n",
      "  Processed 10,000/23,888 customers\n",
      "  Processed 15,000/23,888 customers\n",
      "  Processed 20,000/23,888 customers\n",
      "‚úÖ First-conversion features calculation complete\n",
      "\n",
      "üîç VALIDATION REPORT:\n",
      "   Total customers: 23,888\n",
      "   First converters: 9,458 (39.6%)\n",
      "   Never converters: 14,430\n",
      "\n",
      "üìä Distribution check:\n",
      "   Converters with 0 historical quotes: 7,105\n",
      "   Non-converters with 0 historical quotes: 10,488\n",
      "   Avg historical quotes - Converters: 0.3\n",
      "   Avg historical quotes - Non-converters: 0.4\n",
      "\n",
      "‚úÖ LEAKAGE PREVENTION CONFIRMED:\n",
      "   1. No conversion rate features (would leak)\n",
      "   2. Same feature calculation for all customers\n",
      "   3. No post-first-conversion data used\n",
      "   4. Features use only pre-prediction-point data\n",
      "   5. Target variable name kept as 'converted' for compatibility\n",
      "23888\n",
      "================================================================================\n",
      "CREATING BRAND FEATURES (ULTRA-FAST, LEAKAGE-FREE)\n",
      "================================================================================\n",
      "Processing 34,014 quotes for 23,888 customers\n",
      "üë• Single groupby aggregation...\n",
      "  Processing 23,888 customers with brand data\n",
      "‚ö° Vectorized feature calculation...\n",
      "‚úÖ Vectorized calculations complete\n",
      "üìù Creating final DataFrame...\n",
      "\n",
      "‚úÖ Created 8 brand features\n",
      "   Total customers: 23,888\n",
      "   With brand data: 23,888\n",
      "\n",
      "üìä FEATURE SUMMARY:\n",
      "--------------------------------------------------\n",
      "brand_loyalty_index       : mean = 0.923\n",
      "brand_switches            : mean = 0.186\n",
      "brand_consistency         : mean = 0.840\n",
      "prefers_premium_brand     : mean = 0.240\n",
      "prefers_budget_brand      : mean = 0.313\n",
      "23888\n",
      "================================================================================\n",
      "CREATING MODEL FEATURES (HYPER-FAST)\n",
      "================================================================================\n",
      "Processing 23,888 customers\n",
      "üë• Grouping and preprocessing...\n",
      "  Processing 23,888 customers\n",
      "‚ö° Hyper-fast calculations...\n",
      "‚úÖ Calculations complete\n",
      "üìù Creating final DataFrame...\n",
      "\n",
      "‚úÖ Created 14 model features\n",
      "   Customers: 23,888\n",
      "23888\n",
      "================================================================================\n",
      "CREATING LEAKAGE-FREE MARKET FEATURES\n",
      "================================================================================\n",
      "üö® FORCE REMOVING LEAKY BRANDS\n",
      "Found 23 leaky brands\n",
      "Found 23 near-perfect brands (‚â•99% or ‚â§1%)\n",
      "Total suspicious brands to remove: 23\n",
      "  - KERMI: 0.0% (1.0 quotes)\n",
      "  - GIACOMINI: 0.0% (1.0 quotes)\n",
      "  - GENERIC: 100.0% (1.0 quotes)\n",
      "  - ASTRAL POOL: 0.0% (1.0 quotes)\n",
      "  - AIRZONE: 0.0% (1.0 quotes)\n",
      "  - WILO: 0.0% (1.0 quotes)\n",
      "  - JACOB DELAFON: 0.0% (2.0 quotes)\n",
      "  - WEISHAUPT: 0.0% (1.0 quotes)\n",
      "  - ARTENSE: 0.0% (4.0 quotes)\n",
      "  - HENRAD: 0.0% (2.0 quotes)\n",
      "Replaced 37 quotes with 'SUSPICIOUS_BRAND'\n",
      "Remaining leaky brands after removal: 0\n",
      "Processing 34,014 quotes for 23,888 customers\n",
      "Using ALL quotes for market features (brand preferences are stable)\n",
      "  Quotes for feature calculation: 34,014\n",
      "üë• Grouping customer data...\n",
      "  Processing 23,888 customers with brand data\n",
      "‚ö° Calculating leakage-free features...\n",
      "‚úÖ Calculations complete\n",
      "üéØ Adding target variable...\n",
      "\n",
      "‚úÖ Created 7 leakage-free market features\n",
      "   Total customers: 23,888\n",
      "   With brand data: 23,888\n",
      "   Converters: 9,458 (39.6%)\n",
      "\n",
      "üîç DEBUG: Checking market_data_available distribution:\n",
      "   Converters with brand data: 9,458/9,458 (100.0%)\n",
      "   Non-converters with brand data: 14,430/14,430 (100.0%)\n",
      "   Ratio (should be ~similar): 1.000 vs 1.000\n",
      "\n",
      "üîç Feature correlations with target (should be < 0.3 for no leakage):\n",
      "   ‚úÖ premium_brand_ratio      : -0.038\n",
      "   ‚úÖ budget_brand_ratio       : 0.024\n",
      "   ‚úÖ brand_consistency        : -0.038\n",
      "   ‚úÖ brand_diversity_index    : 0.038\n",
      "23888\n",
      "================================================================================\n",
      "CREATING EQUIPMENT UPGRADE PATH FEATURES (LEAKAGE-SAFE VERSION)\n",
      "================================================================================\n",
      "üîç Applying first conversion filtering...\n",
      "‚ö†Ô∏è  No first_purchase_dates provided - using all data (RISKY!)\n",
      "Processing equipment upgrade data for 23,888 customers\n",
      "üîç Ensuring chronological order to prevent leakage...\n",
      "‚úÖ Data sorted chronologically by customer and date\n",
      "\n",
      "üìä Applying complexity and seasonality mappings...\n",
      "Customers with equipment data: 23,888\n",
      "\n",
      "üîß Computing group-by aggregations...\n",
      "üîÑ Merging aggregated features...\n",
      "‚ö° Computing derived features...\n",
      "‚ùå Removing leaking features: upgrade_trajectory_score, has_upgrade, has_downgrade\n",
      "üßπ Cleaning up missing values...\n",
      "\n",
      "‚úÖ Created 30 SAFE equipment upgrade features\n",
      "   Samples: 23,888 customers\n",
      "   REMOVED: upgrade_trajectory_score, has_upgrade, has_downgrade (potential leakage)\n",
      "   FIRST CONVERSION MODE: DISABLED\n",
      "\n",
      "üìä KEY UPGRADE FEATURES SUMMARY:\n",
      "------------------------------------------------------------\n",
      "equipment_family_consistency        : mean=0.929, std=0.256, non-zero=92.9%\n",
      "seasonal_equipment_mix              : mean=0.048, std=0.209, non-zero=5.1%\n",
      "equipment_maturity_level            : mean=0.321, std=0.087, non-zero=100.0%\n",
      "equipment_variety_index             : mean=0.071, std=0.256, non-zero=7.1%\n",
      "has_multi_season                    : mean=0.051, std=0.219, non-zero=5.1%\n",
      "early_technology_adopter            : mean=0.198, std=0.398, non-zero=19.8%\n",
      "23888\n",
      "================================================================================\n",
      "CREATING SOLUTION COMPLEXITY FEATURES (VECTORIZED)\n",
      "================================================================================\n",
      "üîç Applying first conversion filtering...\n",
      "‚ö†Ô∏è  No first_purchase_dates provided - using all data\n",
      "Processing solution complexity for 23,888 customers\n",
      "üîç Ensuring chronological order...\n",
      "‚úÖ Data sorted chronologically by customer and date\n",
      "\n",
      "üìä Preparing data for vectorized processing...\n",
      "Processing 34,014 equipment records\n",
      "üîß Creating system type indicators...\n",
      "üìà Computing aggregated features...\n",
      "‚ö° Calculating complexity scores...\n",
      "üå± Calculating energy efficiency...\n",
      "üîó Computing system integration...\n",
      "üèÜ Determining primary systems...\n",
      "üîÑ Merging all features...\n",
      "üìä Calculating sophistication tiers...\n",
      "\n",
      "‚úÖ Created 27 solution complexity features\n",
      "   Samples: 23,888 customers\n",
      "   FIRST CONVERSION MODE: DISABLED\n",
      "\n",
      "üìä SOLUTION COMPLEXITY FEATURES SUMMARY:\n",
      "------------------------------------------------------------\n",
      "multi_system_count                  : mean=0.936, std=0.313\n",
      "solution_complexity_score           : mean=2.717, std=0.940\n",
      "has_complete_heating_solution       : mean=0.000, std=0.000\n",
      "energy_efficiency_score             : mean=0.800, std=1.858\n",
      "system_integration_level            : mean=0.017, std=0.123\n",
      "primary_system_dominance            : mean=0.968, std=0.119\n",
      "sophistication_score                : mean=0.972, std=0.756\n",
      "future_proofing_score               : mean=0.054, std=0.108\n",
      "23888\n",
      "================================================================================\n",
      "CREATING TIMELINE FEATURES (mode: first_conversion)\n",
      "================================================================================\n",
      "Using date column: 'dt_creation_devis'\n",
      "Processing 23,888 customers\n",
      "\n",
      "üîß NO FILTERING: Using ALL quotes for timeline features\n",
      "   (Like market features, timeline patterns are stable characteristics)\n",
      "   Quotes for features: 34,014\n",
      "\n",
      "üéØ Adding target variable...\n",
      "\n",
      "üö® VERIFICATION: Data availability (should be ~100% for both)\n",
      "  Converters with data: 100.0%\n",
      "  Non-converters with data: 100.0%\n",
      "  Difference: 0.0% (should be < 10%)\n",
      "\n",
      "‚úÖ Created 20 timeline features\n",
      "   Total customers: 23,888\n",
      "   Converters: 9,458 (39.6%)\n",
      "23888\n",
      "================================================================================\n",
      "CREATING ADVANCED TIMELINE FEATURES (mode: first_conversion)\n",
      "================================================================================\n",
      "Initial data: 34,014 quotes for 23,888 customers\n",
      "\n",
      "üîß NO FILTERING: Using ALL quotes for advanced timeline features\n",
      "   (Like basic timeline and market features)\n",
      "   Quotes for features: 34,014\n",
      "\n",
      "üéØ Adding target variable...\n",
      "\n",
      "‚úÖ Created advanced timeline features for 23,888 customers\n",
      "   With data: 23,888\n",
      "   Converters: 9,458 (39.6%)\n",
      "\n",
      "üö® VERIFICATION: Data balance\n",
      "   Converters with data: 100.0%\n",
      "   Non-converters with data: 100.0%\n",
      "   Difference: 0.0%\n",
      "23888\n",
      "üîç Applying first conversion filtering...\n",
      "‚ö†Ô∏è  No first_purchase_dates provided - using all data\n",
      "‚úÖ Created 3 LEAKAGE-SAFE commercial features\n",
      "   REMOVED: total_quotes, unique_roles, senior_commercial_count (potential leakage)\n",
      "   FIRST CONVERSION MODE: DISABLED\n",
      "23888\n",
      "================================================================================\n",
      "CREATING CUSTOMER-LEVEL PROCESS FEATURES - LEAKAGE SAFE\n",
      "================================================================================\n",
      "Processing 34,014 quotes for 23,888 customers\n",
      "üë• Grouping by customer...\n",
      "  Processing 23,888 customers\n",
      "‚ö° Calculating customer-level features...\n",
      "üìù Creating final DataFrame...\n",
      "\n",
      "‚úÖ Created 10 customer-level process features for 23,888 customers\n",
      "\n",
      "üìä FEATURE SUMMARY:\n",
      "--------------------------------------------------\n",
      "process_adoption_rate          : mean = 0.567, std = 0.489\n",
      "has_ever_used_process          : 57.9% positive\n",
      "process_consistency            : mean = 0.982, std = 0.119\n",
      "process_preference             : 57.9% positive\n",
      "23888\n",
      "Creating features from raw data with error correction...\n",
      "  ‚Üí Creating ALL feature sets...\n",
      "================================================================================\n",
      "CREATING DECISION SPEED FEATURES (CUSTOMER-LEVEL) - LEAKAGE SAFE\n",
      "================================================================================\n",
      "Processing 34,014 quotes for 23,888 customers\n",
      "üë• Single groupby aggregation...\n",
      "  Processing 23,888 customers\n",
      "‚ö° Vectorized feature calculation...\n",
      "‚úÖ Vectorized calculations complete\n",
      "üìù Creating final DataFrame...\n",
      "\n",
      "‚úÖ Created 4 decision speed features for 23,888 customers\n",
      "================================================================================\n",
      "CREATING INTERACTION FEATURES (CUSTOMER-LEVEL) - LEAKAGE SAFE\n",
      "================================================================================\n",
      "Processing 34,014 quotes for 23,888 customers\n",
      "üë• Single groupby aggregation...\n",
      "  Processing 23,888 customers\n",
      "‚ö° Vectorized feature calculation...\n",
      "‚úÖ Vectorized calculations complete\n",
      "üìù Creating final DataFrame...\n",
      "\n",
      "‚úÖ Created 5 interaction features for 23,888 customers\n",
      "================================================================================\n",
      "CREATING VECTORIZED PURCHASE VELOCITY FEATURES - TRAINING MODE\n",
      "================================================================================\n",
      "üìä Processing 34,014 quotes for 23,888 customers\n",
      "‚úÖ Created 7 vectorized features for 23,888 customers\n",
      "================================================================================\n",
      "CREATING DECISION CONSISTENCY FEATURES - TRAINING MODE\n",
      "================================================================================\n",
      "‚úÖ Created 8 features for 23,888 customers\n",
      "  ‚Üí Creating interactions and scores...\n",
      "  ‚Üí Creating error pattern interactions...\n",
      "    Created: wasted_effort_risk (targets False Positives)\n",
      "    Created: quick_converter_potential (targets False Negatives)\n",
      "    Created: serious_high_value_buyer (targets True Positives)\n",
      "    Created: ideal_buyer_pattern (enhances True Positives)\n",
      "    Created: loyal_repeat_potential (enhances True Positives)\n",
      "    Created: indecisive_shopper (targets False Positives)\n",
      "    Created: purchase_ready (targets True Positives)\n",
      "    Total: Created 8 error pattern interactions\n",
      "  ‚Üí Creating business-ready scores...\n",
      "    Priority component: brief_engagement_risk (weight: 0.30)\n",
      "    Priority component: inverse_tire_kicker (weight: 0.25)\n",
      "    Priority component: decision_urgency_score (weight: 0.20)\n",
      "    Priority component: budget_clarity_score (weight: 0.15)\n",
      "    Created: sales_priority_score (for sales team action)\n",
      "    Confidence component: solution_stability_score (weight: 0.25)\n",
      "    Confidence component: brand_loyalty_score (weight: 0.20)\n",
      "    Confidence component: engagement_consistency (weight: 0.20)\n",
      "    Confidence component: price_consistency_score (weight: 0.35)\n",
      "    Created: conversion_confidence_score (model confidence level)\n",
      "    Readiness component: optimal_timing_score (weight: 0.25)\n",
      "    Readiness component: decision_maturity_index (weight: 0.20)\n",
      "    Readiness component: purchase_velocity_index (weight: 0.20)\n",
      "    Readiness component: equipment_focus_score (weight: 0.15)\n",
      "    Readiness component: quote_frequency_score (weight: 0.10)\n",
      "    Readiness component: inverse_needs_refinement (weight: 0.10)\n",
      "    Created: buyer_readiness_score (overall purchase readiness)\n",
      "    Total: Created 4 business-ready scores\n",
      "  ‚Üí SINGLE MERGE of all features...\n",
      "  ‚Üí Adding conversion target...\n",
      "‚úÖ Created 60 features for 23888 customers\n",
      "  ‚Üí Added 2 dominant features (standard mode)\n",
      "    New: quote_efficiency_dominant, is_tire_kicker_binary\n",
      "23888\n",
      "\n",
      "üîß ENCODING & PREPARING FOR MODELING...\n",
      "  Preparing Customer Features...\n",
      "  Features: 14, Samples: 23888\n",
      "\n",
      "üîß ENCODING & PREPARING FOR MODELING...\n",
      "  Preparing Sequence Features...\n",
      "  Features: 29, Samples: 23888\n",
      "\n",
      "üîß ENCODING & PREPARING FOR MODELING...\n",
      "  Preparing New Features...\n",
      "  Features: 209, Samples: 23888\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from etl.util import prepare_dataset_without_leakage\n",
    "from ml_features.features import prepare_features\n",
    "from ml_features.customer_features import create_customer_features\n",
    "from ml_features.sequence_features  import create_sequence_features\n",
    "from ml_features.brand_features import create_brand_features\n",
    "from ml_features.model_features import create_model_features\n",
    "from ml_features.market_features import create_market_features\n",
    "from ml_features.equipment_features import create_equipment_features\n",
    "from ml_features.solution_complexity_features import create_solution_complexity_features\n",
    "from ml_features.timeline_features import create_timeline_features, create_advanced_timeline_features, create_timeline_interaction_features\n",
    "from ml_features.role_features import create_commercial_role_features\n",
    "from ml_features.process_features import create_process_features\n",
    "from ml_features.correction_features import create_correction_features\n",
    "from ml_training.train_rf import train_rf\n",
    "from ml_evaluation.dashboard import model_evaluation_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load original clean quote data\n",
    "df_quotes = pd.read_csv('cleaned_quote_data.csv')\n",
    "df_quotes['dt_creation_devis'] = pd.to_datetime(df_quotes['dt_creation_devis'])\n",
    "\n",
    "print(f\"\\nüìä Original quote data: {len(df_quotes):,} quotes from {df_quotes['numero_compte'].nunique():,} customers\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STRATEGY: CREATE FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create feature list\n",
    "feature_funcs = [create_customer_features, create_sequence_features, create_brand_features, \n",
    "                 create_model_features, create_market_features,\n",
    "                 create_equipment_features, create_solution_complexity_features,\n",
    "                 create_timeline_features, create_advanced_timeline_features,\n",
    "                 create_commercial_role_features, create_process_features, create_correction_features]\n",
    "\n",
    "\n",
    "new_df = feature_funcs[0](df_quotes)\n",
    "customer_df = new_df\n",
    "for func in feature_funcs[1:]:\n",
    "    new_df_ = func(df_quotes)\n",
    "\n",
    "    new_df = pd.merge(new_df, new_df_, on='numero_compte', how='left', suffixes=('_dup', ''))\n",
    "    new_df = new_df.drop(columns=[x for x in new_df.columns if '_dup' in x], errors='ignore')\n",
    "    print(len(new_df))\n",
    "    if func == create_sequence_features: sequence_df = new_df\n",
    "\n",
    "\n",
    "# Now it's clear which column is which\n",
    "y_new = new_df['converted']  # From sequence features\n",
    "y_sequence = sequence_df['converted']  # From sequence features\n",
    "y_customer = customer_df['converted']  # From customer features\n",
    "\n",
    "# For modeling, use the sequence version\n",
    "X_customer = customer_df.drop(columns=['numero_compte', 'converted'], errors='ignore')\n",
    "X_customer_clean, y_customer_clean = prepare_features(X_customer, y_customer, \"Customer Features\")\n",
    "\n",
    "columns_to_drop =  [x for x in sequence_df.columns if '_seq' in x]\n",
    "columns_to_drop.extend(['numero_compte', 'converted'])\n",
    "X_sequence = sequence_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "X_sequence_clean, y_sequence_clean = prepare_features(X_sequence, y_sequence, \"Sequence Features\")\n",
    "\n",
    "new_df = create_timeline_interaction_features(new_df)\n",
    "X_new = new_df.drop(columns=['numero_compte', 'converted'], errors='ignore')\n",
    "X_new_clean, y_new_clean = prepare_features(X_new, y_new, \"New Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13f24c30-4793-4902-969d-0d59aff557d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CREATING SAFE DL-OPTIMIZED FEATURES (V2)\n",
      "================================================================================\n",
      "üìä Input shape: (23888, 209)\n",
      "üìã Found 209 numeric columns\n",
      "\n",
      "üîß Step 1: Scaling features to reasonable range...\n",
      "\n",
      "üîß Step 2: Adding safe transformations to ALL numeric features...\n",
      "\n",
      "üîß Step 3: Adding safe interactions...\n",
      "    ‚úì Added interaction: std_days_between_quotes / price_trajectory\n",
      "\n",
      "üîß Step 4: Clipping all features to safe range...\n",
      "\n",
      "‚úÖ SAFE DL Features Created:\n",
      "  Original: 209 features\n",
      "  Final: 837 features\n",
      "  Added: 628 new features\n",
      "\n",
      "üìä Safe value ranges:\n",
      "  Min: -10.00\n",
      "  Max: 10.00\n",
      "  Mean: 0.15\n"
     ]
    }
   ],
   "source": [
    "from dl_training.train import train_advanced_dl_model\n",
    "from dl_features.features import create_dl_specific_features\n",
    "\n",
    "X_dl_optimized, y_dl = create_dl_specific_features(X_new_clean, y_new_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8750bce-a75b-4b61-8fa3-ab066e879631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 0 focused features\n"
     ]
    }
   ],
   "source": [
    "def create_focused_features(X, y):\n",
    "    \"\"\"Create new features focused on what matters most\"\"\"\n",
    "    X_focused = X.copy()\n",
    "    \n",
    "    # Double down on conversion rate features\n",
    "    if 'avg_recent_conversion_rate' in X.columns:\n",
    "        # More transformations of the most important feature\n",
    "        X_focused['conversion_rate_exp'] = np.exp(X['avg_recent_conversion_rate'].clip(-10, 10))\n",
    "        X_focused['conversion_rate_power3'] = X['avg_recent_conversion_rate'] ** 3\n",
    "        X_focused['conversion_rate_sigmoid'] = 1 / (1 + np.exp(-X['avg_recent_conversion_rate']))\n",
    "    \n",
    "    # Agency-conversion interactions\n",
    "    if 'main_agency_log' in X.columns and 'avg_recent_conversion_rate' in X.columns:\n",
    "        X_focused['agency_conversion_interaction'] = X['main_agency_log'] * X['avg_recent_conversion_rate']\n",
    "    \n",
    "    # Discount-conversion interactions  \n",
    "    if 'avg_discount_pct_abs_sqrt' in X.columns and 'avg_recent_conversion_rate' in X.columns:\n",
    "        X_focused['discount_conversion_interaction'] = X['avg_discount_pct_abs_sqrt'] * X['avg_recent_conversion_rate']\n",
    "    \n",
    "    # Price-conversion interactions\n",
    "    price_cols = [c for c in X.columns if 'price' in c.lower() and 'conversion' not in c.lower()]\n",
    "    if price_cols and 'avg_recent_conversion_rate' in X.columns:\n",
    "        for price_col in price_cols[:3]:  # Top 3 price features\n",
    "            X_focused[f'{price_col}_conversion_interaction'] = X[price_col] * X['avg_recent_conversion_rate']\n",
    "    \n",
    "    print(f\"Added {X_focused.shape[1] - X.shape[1]} focused features\")\n",
    "    return X_focused\n",
    "\n",
    "# Create focused features\n",
    "X_focused = create_focused_features(X_dl_optimized, y_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36af9f88-80e0-4252-9805-00e31428bd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 3 region-focused features\n"
     ]
    }
   ],
   "source": [
    "def enhance_region_features(X):\n",
    "    \"\"\"Enhance region-related features since they're most important\"\"\"\n",
    "    X_enhanced = X.copy()\n",
    "    \n",
    "    if 'main_region' in X.columns:\n",
    "        # Region is categorical but encoded as numeric - create better features\n",
    "        # Create region clusters if you have more info\n",
    "        X_enhanced['region_is_popular'] = (X['main_region'] == X['main_region'].mode()[0]).astype(int)\n",
    "        \n",
    "        # Region interactions with price\n",
    "        if 'avg_current_price' in X.columns:\n",
    "            X_enhanced['region_price_interaction'] = X['main_region'] * X['avg_current_price']\n",
    "        \n",
    "        # Region interactions with discount\n",
    "        if 'avg_discount_pct' in X.columns:\n",
    "            X_enhanced['region_discount_interaction'] = X['main_region'] * X['avg_discount_pct']\n",
    "    \n",
    "    print(f\"Added {X_enhanced.shape[1] - X.shape[1]} region-focused features\")\n",
    "    return X_enhanced\n",
    "\n",
    "X_region_enhanced = enhance_region_features(X_focused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d74b509-856c-4443-8e0a-76bd29937a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 0 region-focused features\n"
     ]
    }
   ],
   "source": [
    "def enhance_discount_features(X):\n",
    "    \"\"\"Discounts are #2 important - enhance them\"\"\"\n",
    "    X_enhanced = X.copy()\n",
    "    \n",
    "    if 'avg_discount_pct' in X.columns:\n",
    "        # Discount tiers\n",
    "        X_enhanced['discount_tier'] = pd.cut(\n",
    "            X['avg_discount_pct'], \n",
    "            bins=[-np.inf, 0, 5, 10, 20, np.inf],\n",
    "            labels=['negative', 'small', 'medium', 'large', 'very_large']\n",
    "        ).cat.codes\n",
    "        \n",
    "        # Is there any discount?\n",
    "        X_enhanced['has_discount'] = (X['avg_discount_pct'] > 0).astype(int)\n",
    "        \n",
    "        # Discount effectiveness (interact with price)\n",
    "        if 'avg_current_price' in X.columns:\n",
    "            X_enhanced['discount_price_ratio'] = X['avg_discount_pct'] / (X['avg_current_price'] + 1)\n",
    "    \n",
    "    print(f\"Added {X_enhanced.shape[1] - X.shape[1]} discount-focused features\")\n",
    "    return X_enhanced\n",
    "\n",
    "X_discount_enhanced = enhance_region_features(X_region_enhanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f627bf8b-6445-425e-ac05-f76404378a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Training ADVANCED DL Model...\n",
      "üîß Normalizing features for DL...\n",
      "  Before: min=-20.00, max=20.00\n",
      "  Before: mean=0.14, std=0.80\n",
      "  After: min=-6.17, max=6.60\n",
      "  After: mean=0.54, std=1.78\n",
      "  Normalization complete!\n",
      "üîß Normalizing features for DL...\n",
      "  Before: min=-20.00, max=13.58\n",
      "  Before: mean=0.15, std=0.80\n",
      "  After: min=-2.69, max=13.72\n",
      "  After: mean=0.86, std=2.09\n",
      "  Normalization complete!\n",
      "  Parameters: 365,833\n",
      "  Model: advanced\n",
      "  Input dim: 840\n",
      "  Parameters: 365,833\n",
      "  Training samples: 19,110\n",
      "  Validation samples: 4,778\n",
      "  ‚úì Epoch 1: Loss=0.9000, Val AUC=0.6598\n",
      "  ‚úì Epoch 2: Loss=0.8266, Val AUC=0.6832\n",
      "  ‚úì Epoch 3: Loss=0.7953, Val AUC=0.7052\n",
      "  ‚úì Epoch 4: Loss=0.7723, Val AUC=0.7072\n",
      "  ‚úì Epoch 5: Loss=0.7627, Val AUC=0.7172\n",
      "  ‚úì Epoch 6: Loss=0.7525, Val AUC=0.7188\n",
      "  ‚úì Epoch 11: Loss=0.7202, Val AUC=0.7188\n",
      "  ‚úì Epoch 12: Loss=0.7172, Val AUC=0.7225\n",
      "  ‚èπÔ∏è Early stopping at epoch 37\n",
      "\n",
      "‚úÖ Training Complete!\n",
      "  Best Val AUC: 0.7225\n"
     ]
    }
   ],
   "source": [
    "model, auc = train_advanced_dl_model(\n",
    "    X_discount_enhanced, y_dl \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dafe5e90-a78e-43e4-84bc-6838621ea9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GRADIENT-BASED INPUT FEATURE IMPORTANCE\n",
      "================================================================================\n",
      "\n",
      "Top 20 input features by gradient magnitude:\n",
      "  had_historical_quotes_tanh               | Gradient: 0.401832\n",
      "  total_historical_quotes                  | Gradient: 0.376201\n",
      "  total_historical_quotes_abs_sqrt         | Gradient: 0.345707\n",
      "  total_historical_quotes_log              | Gradient: 0.330201\n",
      "  had_historical_quotes_log                | Gradient: 0.325407\n",
      "  total_historical_quotes_tanh             | Gradient: 0.278469\n",
      "  had_historical_quotes_abs_sqrt           | Gradient: 0.272736\n",
      "  region_discount_interaction              | Gradient: 0.251772\n",
      "  total_quotes                             | Gradient: 0.240863\n",
      "  avg_price_tanh                           | Gradient: 0.216465\n",
      "  _total_quotes                            | Gradient: 0.208724\n",
      "  avg_discount_pct_abs_sqrt                | Gradient: 0.199091\n",
      "  avg_discount_abs_sqrt                    | Gradient: 0.198241\n",
      "  peak_weekday                             | Gradient: 0.182736\n",
      "  avg_price                                | Gradient: 0.172630\n",
      "  min_price                                | Gradient: 0.156061\n",
      "  is_tire_kicker_binary_log                | Gradient: 0.151432\n",
      "  quote_count                              | Gradient: 0.150094\n",
      "  model_type_concentration_log             | Gradient: 0.148337\n",
      "  month_8_count_abs_sqrt                   | Gradient: 0.148001\n",
      "\n",
      "üîç FEATURE CATEGORY ANALYSIS:\n",
      "  Price     : 62 features | Avg importance: 0.054141\n",
      "  Quote     : 85 features | Avg importance: 0.073156\n",
      "  Day       : 45 features | Avg importance: 0.042969\n",
      "  Average   : 56 features | Avg importance: 0.064476\n",
      "  Std Dev   : 39 features | Avg importance: 0.037601\n",
      "  Trend     :  4 features | Avg importance: 0.039108\n",
      "  Ratio     : 81 features | Avg importance: 0.037517\n",
      "  Log       : 212 features | Avg importance: 0.044473\n",
      "  Tanh      : 209 features | Avg importance: 0.043079\n",
      "  Sqrt      : 209 features | Avg importance: 0.043811\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def analyze_attention_weights(model, X_sample):\n",
    "    \"\"\"\n",
    "    Analyze which HIDDEN features the attention mechanism focuses on\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a sample batch\n",
    "    if isinstance(X_sample, pd.DataFrame):\n",
    "        X_tensor = torch.FloatTensor(X_sample.values[:100])  # First 100 samples\n",
    "    else:\n",
    "        X_tensor = torch.FloatTensor(X_sample[:100])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Forward pass through network\n",
    "        features = model.net(X_tensor)  # Shape: [100, 64] (hidden features)\n",
    "        attention_weights = model.attention(features)  # Shape: [100, 64]\n",
    "        \n",
    "        # Get average attention per HIDDEN feature\n",
    "        avg_attention = attention_weights.mean(dim=0).squeeze().numpy()\n",
    "    \n",
    "    print(f\"Input features: {X_sample.shape[1]}\")\n",
    "    print(f\"Hidden features: {features.shape[1]}\")\n",
    "    \n",
    "    # Create importance DataFrame for HIDDEN features\n",
    "    importance_df = pd.DataFrame({\n",
    "        'hidden_feature_idx': list(range(len(avg_attention))),  # FIX: list of ints\n",
    "        'attention_weight': avg_attention\n",
    "    }).sort_values('attention_weight', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ATTENTION-BASED HIDDEN FEATURE IMPORTANCE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTop 20 hidden features by attention weight:\")\n",
    "    for i, row in importance_df.head(20).iterrows():\n",
    "        # FIX: Convert to int for formatting\n",
    "        feat_idx = int(row['hidden_feature_idx'])\n",
    "        print(f\"  Hidden feature {feat_idx:3d} | Attention: {row['attention_weight']:.4f}\")\n",
    "    \n",
    "    # Check if attention is actually working\n",
    "    variance = importance_df['attention_weight'].var()\n",
    "    print(f\"\\nüîç Attention variance: {variance:.6f}\")\n",
    "    if variance < 0.001:\n",
    "        print(\"‚ö†Ô∏è  WARNING: Attention weights are nearly identical!\")\n",
    "    else:\n",
    "        print(\"‚úÖ GOOD: Attention weights vary across features\")\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# BETTER: Analyze which INPUT features matter using gradients\n",
    "def analyze_input_feature_importance(model, X_sample):\n",
    "    \"\"\"\n",
    "    Analyze which INPUT features matter using gradient-based importance\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert to tensor with gradient tracking\n",
    "    X_tensor = torch.FloatTensor(X_sample.values[:100])\n",
    "    X_tensor.requires_grad = True\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(X_tensor)\n",
    "    \n",
    "    # Create dummy target for gradient computation\n",
    "    dummy_target = torch.ones_like(output)\n",
    "    \n",
    "    # Backward pass to get gradients w.r.t inputs\n",
    "    model.zero_grad()\n",
    "    output.backward(dummy_target)\n",
    "    \n",
    "    # Get average absolute gradient per INPUT feature\n",
    "    gradients = X_tensor.grad.abs().mean(dim=0).numpy()\n",
    "    \n",
    "    # Create importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X_sample.columns.tolist(),\n",
    "        'gradient_importance': gradients\n",
    "    }).sort_values('gradient_importance', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GRADIENT-BASED INPUT FEATURE IMPORTANCE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTop 20 input features by gradient magnitude:\")\n",
    "    for i, row in importance_df.head(20).iterrows():\n",
    "        print(f\"  {row['feature']:40s} | Gradient: {row['gradient_importance']:.6f}\")\n",
    "    \n",
    "    # Also show feature categories\n",
    "    print(f\"\\nüîç FEATURE CATEGORY ANALYSIS:\")\n",
    "    \n",
    "    categories = {\n",
    "        'Price': ['price'],\n",
    "        'Quote': ['quote'],\n",
    "        'Day': ['day'],\n",
    "        'Average': ['avg_'],\n",
    "        'Std Dev': ['std_'],\n",
    "        'Trend': ['trend'],\n",
    "        'Ratio': ['ratio', 'div', 'per'],\n",
    "        'Log': ['log'],\n",
    "        'Squared': ['squared'],\n",
    "        'Tanh': ['tanh'],\n",
    "        'Sqrt': ['sqrt']\n",
    "    }\n",
    "    \n",
    "    for cat_name, keywords in categories.items():\n",
    "        cat_features = [f for f in importance_df['feature'] \n",
    "                       if any(kw in f.lower() for kw in keywords)]\n",
    "        \n",
    "        if cat_features:\n",
    "            cat_importance = importance_df[\n",
    "                importance_df['feature'].isin(cat_features)\n",
    "            ]['gradient_importance'].mean()\n",
    "            \n",
    "            print(f\"  {cat_name:10s}: {len(cat_features):2d} features | \"\n",
    "                  f\"Avg importance: {cat_importance:.6f}\")\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Usage\n",
    "importance_df = analyze_input_feature_importance(model, X_region_enhanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9610cdcc-cc3b-4453-a7a9-29d5b370319f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GRADIENT-BASED FEATURE IMPORTANCE\n",
      "================================================================================\n",
      "\n",
      "Top 20 features by gradient magnitude:\n",
      "  had_historical_quotes_tanh               | Gradient: 0.401832\n",
      "  total_historical_quotes                  | Gradient: 0.376201\n",
      "  total_historical_quotes_abs_sqrt         | Gradient: 0.345707\n",
      "  total_historical_quotes_log              | Gradient: 0.330201\n",
      "  had_historical_quotes_log                | Gradient: 0.325407\n",
      "  total_historical_quotes_tanh             | Gradient: 0.278469\n",
      "  had_historical_quotes_abs_sqrt           | Gradient: 0.272736\n",
      "  region_discount_interaction              | Gradient: 0.251772\n",
      "  total_quotes                             | Gradient: 0.240863\n",
      "  avg_price_tanh                           | Gradient: 0.216465\n",
      "  _total_quotes                            | Gradient: 0.208724\n",
      "  avg_discount_pct_abs_sqrt                | Gradient: 0.199091\n",
      "  avg_discount_abs_sqrt                    | Gradient: 0.198241\n",
      "  peak_weekday                             | Gradient: 0.182736\n",
      "  avg_price                                | Gradient: 0.172630\n",
      "  min_price                                | Gradient: 0.156061\n",
      "  is_tire_kicker_binary_log                | Gradient: 0.151432\n",
      "  quote_count                              | Gradient: 0.150094\n",
      "  model_type_concentration_log             | Gradient: 0.148337\n",
      "  month_8_count_abs_sqrt                   | Gradient: 0.148001\n"
     ]
    }
   ],
   "source": [
    "def gradient_based_importance(model, X_sample, y_sample):\n",
    "    \"\"\"\n",
    "    Compute feature importance using gradients (Integrated Gradients-like)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert to tensor\n",
    "    X_tensor = torch.FloatTensor(X_sample.values[:100])\n",
    "    X_tensor.requires_grad = True\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(X_tensor)\n",
    "    \n",
    "    # Create dummy target (we want gradients w.r.t inputs)\n",
    "    dummy_target = torch.ones_like(output)\n",
    "    \n",
    "    # Backward pass to get gradients\n",
    "    model.zero_grad()\n",
    "    output.backward(dummy_target)\n",
    "    \n",
    "    # Get average absolute gradient per feature\n",
    "    gradients = X_tensor.grad.abs().mean(dim=0).numpy()\n",
    "    \n",
    "    # Create importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X_sample.columns.tolist(),\n",
    "        'gradient_importance': gradients\n",
    "    }).sort_values('gradient_importance', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GRADIENT-BASED FEATURE IMPORTANCE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTop 20 features by gradient magnitude:\")\n",
    "    for i, row in importance_df.head(20).iterrows():\n",
    "        print(f\"  {row['feature']:40s} | Gradient: {row['gradient_importance']:.6f}\")\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Usage\n",
    "grad_importance = gradient_based_importance(model, X_region_enhanced, y_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[Poetry] ai-france-hvac",
   "language": "python",
   "name": "ai-france-hvac-poetry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
